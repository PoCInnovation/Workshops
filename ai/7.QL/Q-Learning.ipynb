{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gym --user\n",
    "!pip3 install numpy --user\n",
    "!pip3 install matplotlib --user\n",
    "\n",
    "!pip3 install box2d --user\n",
    "!pip3 install pyvirtualdisplay -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "\n",
    "Dans le pr√©c√©dent workshop nous avions vue les bases du **Reinforcement learning** en resolvant L-Antique Maze via la value function. <br>\n",
    "√áa √©t√© l'occasion de d√©couvrir les diverses notions mathematiques derriere le RL (MDP, VF, etc)\n",
    "\n",
    "Dans ce workshop, vous allez apprendre les bases de la notion du **Q-learning** et decouvrir les environnements Gym.<br>\n",
    "Vous allez pour cela √† la fin de ce workshop r√©soudre un environnement nomm√© [MontainCar](gym.openai.com/envs/MountainCar-v0/).\n",
    "\n",
    "### Packages\n",
    "Importons dans un premier temps les d√©pences suivantes:  \n",
    "-numpy est le package fondamental pour le calcul scientifique avec Python.  \n",
    "-matplotlib  est une librairie connue pour afficher des graphiques en Python.  \n",
    "-[gym](https://pypi.org/project/gym/0.7.4/) est un outil utile lors d'usage de reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from validation_tests import *\n",
    "from pyvirtualdisplay import Display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Prise en main avec l'environnement\n",
    "\n",
    "Avant de commencer l'impl√©mentation du Q-learning, commen√ßons par nous familiariser avec la librairie `gym`.\n",
    "Gym vous permet de tester votre agent dans un environnement. Les environnements fournis sont vari√©s et de diverses complexit√©s.\n",
    "\n",
    "Celui d'aujourd'hui est nomm√© `MountainCar-V0`, il consiste en un vehicule situ√© au creux d'une colline ayant pour but de la franchir.<br>\n",
    "\n",
    "Commen√ßons d√©j√† par charger et afficher notre environnement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "env.seed(1)\n",
    "env.reset()\n",
    "\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tr√®s bien, nous avons affich√© notre environnement.\n",
    "\n",
    "Quelques explications:\n",
    "-  `gym.make()` nous a permis de charger notre environnement\n",
    "-  `env.seed()` nous permet d'avoir les m√™mes r√©sultats\n",
    "-  `env.reset()` r√©initialise l'environnement et retourne l'√©tat de notre env\n",
    "-  `env.render()` nous permet d'afficher notre env\n",
    "-  `env.close()` ferme l'environnement\n",
    "\n",
    "Maintenant voyons comment nous pouvons interagir avec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "env.seed(1)\n",
    "env.reset()\n",
    "\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "for j in range(50):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voil√† qui est plus int√©ressant.\n",
    "\n",
    "Quelques explications:\n",
    "-  `env.action_space.sample()` prend une action au hasard parmi celles possibles\n",
    "-  `state` est votre √©tat apr√®s l'action effectu√©e\n",
    "-  `reward` est la r√©compense re√ßu pour avoir effectu√© cette action\n",
    "-  `done` indique si l'environnement est termin√©\n",
    "\n",
    "Le `state` repr√©sente l'√©tat de votre agent dans son environnement, dans le cas de l'environnement *MountainCar-v0* le `state` est compos√© de deux valeurs: `position` et `velocity`.<br>\n",
    "Comme leur nom l'indique, `position` repr√©sente la position de l'agent dans l'environnement et `velocity` represente sa velocit√© √† un instant $t$.\n",
    "\n",
    "Regardons comment retrouver ses informations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Etat: \", env.observation_space)\n",
    "\n",
    "print(\"low: \", env.observation_space.low)\n",
    "print(\"hight: \", env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D√©cortiquons ces informations ensemble.\n",
    "\n",
    "`Box()` r√©presente un tableau √† N dimensions, il est ici de dimension $(2,)$ ce qui signifie que notre `state` est de dimsensions $(2,)$.<br>\n",
    "Cela correspond bien √† (`position`, `velocity`) comme nous l'avons dit pr√©c√©demment.\n",
    "\n",
    "On a aussi print les `low` et `hight` de ces deux valeurs, ce sont les deux valeurs extr√™mes que peut avoir notre `state`.\n",
    "\n",
    "`[-1.2, 0.6]` correspond √† l'encradrement des valeurs de `position`.<br>\n",
    "`[-0.07, 0.07]` correspond √† l'encradrement des valeurs de `velocity`.\n",
    "\n",
    "Pour en savoir plus vous pouvez voir le repo github de l'environnement [MountainCar](https://github.com/openai/gym/wiki/MountainCar-v0).\n",
    "\n",
    "Maintenant que nous savons comment interpr√©ter les informations de `state`, passons au contr√¥le de notre agent.\n",
    "\n",
    "**Exercice:** Affichez l'espace d'action de notre agent dans son environnement.<br>\n",
    "**Indice:** [doc de Gym](https://gym.openai.com/docs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Action: \", )  # rajoutez votre code (~1 ligne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultat attendu:** `Action:  Discrete(3)`\n",
    "\n",
    "Interpr√©tons ces informations:\n",
    "\n",
    "`Discrete()` signifie que toutes nos actions $\\in N+$.<br>\n",
    "Le `3` correspond au nombre d'actions effectuables par notre agent dans son environnement, dans notre cas ces actions sont: $Reculer$, $Attendre$ et $Avancer$.\n",
    "\n",
    "# 2 - Le Q-learning\n",
    "\n",
    "\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/678cb558a9d59c33ef4810c9618baf34a9577686\">\n",
    "\n",
    "### Nomenclature:\n",
    "\n",
    "- $Q$ est notre `Q-table`\n",
    "- $St$ repr√©sente notre `state` √† un instant $t$\n",
    "- $at$ repr√©sente l'action prise √† un instant $t$\n",
    "- $\\alpha$ (alpha) est le `learning rate`, c'est l'importance que l'on donne au `new state`. <br>Sachant 0 < $\\alpha$ < 1, plus le `learning rate` est √©lev√©, plus on donne de l'importance au `new state`.\n",
    "- $\\gamma$ (gamma) est le `discount factor`, c'est l'importance donn√©e √† un futur reward. <br>Sachant 0 < $\\gamma$ < 1, plus le `discount factor` est √©lev√©, plus notre agent \"pensera\" au long terme.\n",
    "- $\\epsilon$ (epsilon) est le facteur al√©atoire de l'apprentissage<br>Sachant 0 < $\\epsilon$ < 1, plus epsilon est √©lev√©, plus notre agent effectuera des actions al√©atoires.\n",
    "\n",
    "\n",
    "### Algorithme du Q-learning\n",
    "\n",
    "Pour commencer l'utilisateur d√©finit sa Q-table √† des valeur arbitraires.<br>\n",
    "\n",
    "Pour rappel la Q-table contient pour chaque binome `state` $S$ et action $a$, le `reward` qui leur est associ√©.<br>\n",
    "Si l'utilisateur se trouve dans un `state` $St$ et effectue une action $at$ il recevra en `reward` $Q[St, at]$.\n",
    "\n",
    "Une fois la Q-table cr√©√©e on commence l'algorithme:\n",
    "\n",
    "L'utilisateur se trouve √† un `state` $S$.<br>Il r√©cup√®re un nombre al√©atoire $r$, si $r$ < $\\epsilon$ il effectue une action al√©atoire, si $r$ > $\\epsilon$ il effectue alors une action dite `greedy`.<br>\n",
    "Apres avoir effectu√© l'action $a$, l'utilisateur se trouve dans un `state` $S'$ et a re√ßu un `reward` $r$.\n",
    "\n",
    "\n",
    "L'utilisateur va pouvoir mettre √† jour sa Q-table, pour rappel:\n",
    "$Qnew[st, at] = Q[st, at] + \\alpha * (rt  + \\gamma  * maxQ(st+1, a) - Q[st, at])$\n",
    "\n",
    "Au fur et √† mesure que l'utilisateur met √† jour sa Q-table, l'agent saura de mieux en mieux comment optimiser ses action pour obtenir le meilleur `reward` et ainsi r√©soudre son environnement.\n",
    "\n",
    "### La pratique \n",
    "\n",
    "Nous avons vu la th√©orie, maintenant passons √† la pratique.\n",
    "\n",
    "\n",
    "Commen√ßons par initialiser `states` qui contiendra tous les `states` que notre agent pourra rencontrer dans son environnement.\n",
    "\n",
    "Pour rappel:\n",
    "- `[-1.2, 0.6]` correspond √† l'encradrement des valeurs de `position`.<br>\n",
    "- `[-0.07, 0.07]` correspond √† l'encradrement des valeurs de `velocity`.\n",
    "- Notre agent obtient -1 √† chaque action.\n",
    "- Moins notre agent effectu d'action, plus son `reward` sera grand.\n",
    "\n",
    "Une question se pose alors:<br>Il y a une infinit√© de valeurs dans l'encadrement `[-1.2, 0.6]` et l'encadrement `[-0.07, 0.07]`. Comment toutes les stocker ?<br>\n",
    "\n",
    "Cela n'est pas possible. C'est pourquoi nous allons devoir se contenter d'un √©chantillons de ces valeurs.<br>\n",
    "La taille de cette √©chantillon est variable selon la situation, dans notre cas nous allons choisir un √©chantillon de taille $20$.\n",
    "\n",
    "La taille de notre √©chantillon est un vecteur important dans le cas du Q-Learning.\n",
    "Plus l'√©chantillon est grand, plus notre agent sera pr√©cis mais plus il prendra du temps √† tout explorer.\n",
    "Il y a aussi le facteur m√©moire qui rentre en jeu, la taille allou√©e √† notre Q-table augmente de fa√ßon exponentielle:\n",
    "- Un √©chantillon de taille 10 pour 3 actions possibles m√®nera √† une Qtable de shape `((10, 10), 3)` et contenant $300$ valeurs.\n",
    "- Un √©chantillon de taille 15 pour 3 actions possibles m√®nera √† une Qtable de shape `((15, 15), 3)` et contenant $675$ valeurs.\n",
    "- Un √©chantillon de taille 20 pour 3 actions possibles m√®nera √† une Qtable de shape `((20, 20), 3)` et contenant $1200$ valeurs.\n",
    "\n",
    "Ici doubler la taille de l'√©chantillons ne double donc pas la taille de notre Q-table mais la quadruple !\n",
    "\n",
    "Dans notre cas voici √† quoi ressemble notre √©chantillon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATES_SPACE = 20\n",
    "\n",
    "POSITION_SPACE = np.linspace(-1.2, 0.6, STATES_SPACE)\n",
    "VELOCITY_SPACE = np.linspace(-0.07, 0.07, STATES_SPACE)\n",
    "\n",
    "print(f\"POSITION_SPACE: {POSITION_SPACE}\\n\")\n",
    "print(f\"VELOCITY_SPACE: {VELOCITY_SPACE}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La solution la plus √©vidente maintenant serait de stocker dans `states[0][0]` la valeur `(-1.2)` et dans `states[0][1]`la valeur `(0.07)` etc. mais cette solution apporterait d'autres probl√®mes.<br>\n",
    "Exemple: On se retrouve √† avoir dans `states[1][0]` la valeur `-1.10526316` mais notre env ne nous retournera probablement pas la valeur exact `-1.10526316` ce qui demanderai a chaque fois d'arrondir cette derni√®re.\n",
    "\n",
    "Pour contrer cela, au lieu de stocker dans `states` les valeurs de notre √©chantillon, nous allons stocker les index correspondant aux valeurs de notre √©chantillons.<br>\n",
    "Exemple: `states[0]` contiendra alors `(0, 0)` correspondant respectivement √† $-1,2$ et $-0.07$ dans notre √©chantillon.\n",
    "\n",
    "**Exercices:** Compl√©tez la fonction `init_states()` pour qu'elle retourne le tableau `states` de taille `STATES_SPACE` contenant toutes les combinaisons des valeurs repr√©sentants notre echantillon `(observation, velocity)`.<br>\n",
    "**Indices:**\n",
    "- Uttilisez `STATES_SPACE`\n",
    "- `state[1]` correspondant √† `(0, 1)` repr√©sente la $1ere$ valeur de notre √©chantillon de `observation` et la $2eme$ valeur de notre √©chantillon de `velocity`.\n",
    "- `state[20]` correspondant √† `(1, 0)` repr√©sente la $2eme$ valeur de notre √©chantillon de `observation` et la $1ere$ valeur de notre √©chantillon de `velocity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_states():\n",
    "    states = []\n",
    "    # rajoutez votre code (~3 lignes)\n",
    "\n",
    "    \n",
    "    \n",
    "    # fin de votre code\n",
    "    return states\n",
    "\n",
    "assert valide_init_states(init_states(), STATES_SPACE), \"Provided function does not match requirements\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice:** Compl√©tez la fonction `get_indexes()` pour que depuis un `state` donn√© en argument elle retourne les indexes correspondants dans notre √©chantillon.<br>\n",
    "**Indices:** `np.digitize()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indexes(state):\n",
    "    # rajoutez votre code (~2 lignes)\n",
    "    \n",
    "    \n",
    "    # fin de votre code\n",
    "    return (position_index, velocity_index)\n",
    "  \n",
    "assert valide_get_indexes(get_indexes((0, 0)), POSITION_SPACE, VELOCITY_SPACE), \"Provided function does not match requirements\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passons maintenant √† la gestion des actions de notre agent.\n",
    "\n",
    "Lorsque l'on demande √† notre agent d'effectuer une action qui selon lui est optimale, il effectue alors une action dite \"*greedy*\".<br>\n",
    "C'est justement ce que vous allez impl√©menter.\n",
    "\n",
    "**Exercice:** Compl√©tez la fonction `greedy_step()` pour que depuis un `state` donn√© elle retourne l'action optimal √† faire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_SPACE = [0, 1, 2]\n",
    "\n",
    "def greedy_step(Q, state, actions=[0, 1, 2]):\n",
    "    # rajoutez votre code (~2 lignes)\n",
    "    \n",
    "    \n",
    "    # fin de votre code\n",
    "    return action\n",
    "  \n",
    "assert valide_greedy_step(create_testing_Qtable()) == greedy_step(create_testing_Qtable(), (1, 2)), \"Provided function does not match requirements\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementons maintenant l'usage d'epsilon.\n",
    "\n",
    "**Exercice:** Compl√©tez la fonction `take_action` pour qu'elle retourne l'action a faire √† un moment $t$.<br>\n",
    "**Indices:**\n",
    "- Pour $r$ un nombre al√©atoire $\\in$ `N+`, si $r$ < $\\epsilon$ alors l'action effectu√© sera al√©atoire.\n",
    "- `np.random.random()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_action(epsilon, Q, state, actions=[0, 1, 2]):\n",
    "    # rajoutez votre code (~3 lignes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # fin de votre code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez maintenant passer √† l'initialisation de votre Q-table.\n",
    "\n",
    "**Exercice:** Compl√©tez la fonction `init_Qtable() ` pour qu'elle retourne votre Q-table avec toutes ses valeurs initialis√©es √† $0$.<br>\n",
    "**Indice:**\n",
    "- Votre Q-table est de shape `((STATE_SPACE, STATE_SPACE), ACTION_SPACE)`\n",
    "- Qu'est ce qui est de shape `(STATE_SPACE, STATE_SPACE)` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_Qtable(states, action_space):\n",
    "    Q = {}\n",
    "    # rajoutez votre code (~3 lignes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # fin de votre code\n",
    "    return Q\n",
    "  \n",
    "assert valide_init_Qtable(init_states(), ACTION_SPACE) == init_Qtable(init_states(), ACTION_SPACE), \"Provided function does not match requirements\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que votre Q-table est initalis√©e vous devez la mettre √† jour.\n",
    "\n",
    "**Exercice:** Compl√©tez `Qfunction` pour qu'elle retourne la nouvelle valeur de `Q[state, action` selon la *Q-function*.<br>\n",
    "**Indice:** $Qnew[st, at] = Q[st, at] + \\alpha * (rt  + \\gamma  * maxQ(st+1, a) - Q[st, at])$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qfunction(state, action, reward, new_state, alpha, gamma):\n",
    "  # rajoutez votre code (~2 lignes)\n",
    "  \n",
    "  \n",
    "  # fin de votre code\n",
    "  return newQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mise en situation\n",
    "\n",
    "Tout est en place pour passer √† la mise en situation (ou presque, nous y reviendrons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_GAMES = 2_000\n",
    "ALPHA = 0.1\n",
    "GAMMA = 0.99\n",
    "epsilon = 1.0\n",
    "EPS_MIN = 0.1\n",
    "EPS_DECAY = 0.995\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "env.seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "episode_score = 0\n",
    "total_rewards = np.zeros(NUMBER_OF_GAMES)\n",
    "memory = []\n",
    "\n",
    "states = init_states()\n",
    "Q = init_Qtable(states, ACTION_SPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice:** Compl√©tez le code ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for game_number in range(NUMBER_OF_GAMES):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    state = get_indexes(obs)\n",
    "    if game_number % 100 == 0 and game_number > 0:\n",
    "        print('episode ', game_number, 'score ', episode_score, 'epsilon %.3f' % epsilon)\n",
    "    episode_score = 0\n",
    "    while not done:\n",
    "        if (game_number % 500 == 0):\n",
    "            env.render()\n",
    "        # rajoutez votre code (~5 lignes)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # fin de votre code\n",
    "        episode_score += reward\n",
    "    total_rewards[game_number] = episode_score\n",
    "    epsilon = max(EPS_MIN, epsilon * EPS_DECAY)\n",
    "\n",
    "print(f\"average reward: {sum(total_rewards) / len(total_rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rewards = np.zeros(NUMBER_OF_GAMES)\n",
    "\n",
    "for t in range(NUMBER_OF_GAMES):\n",
    "    mean_rewards[t] = np.mean(total_rewards[int(max(0, t - NUMBER_OF_GAMES / 10)): t + 1])\n",
    "\n",
    "plt.plot(mean_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super, on voit que notre agent apprend bien √† r√©soudre son environnement mais n'y a-t-il pas un moyen d'optimiser son apprentissage ?<br>\n",
    "\n",
    "Si ! Et vous avez d√©j√† vu comment: en utilisant le principe d'*experience replay*.<br>\n",
    "On va rajouter une m√©moire √† notre agent pour qu'il puisse s'entrainer plusieurs fois sur des situations qu'il pourrait rencontrer seulement rarement.\n",
    "\n",
    "**Exercice:** Compl√©tez la fonction `add_memory` pour qu'√† chaque appel elle met √† jour la m√©moire avec la nouvelle valeur donn√©e en argument.<br>\n",
    "**Indices:**\n",
    "- Attention √† ne pas d√©passer `memory_size`\n",
    "- Si on doit remplacer une valeur, on remplace la plus ancienne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_memory(memory, state, action, reward, new_state, memory_size=600):\n",
    "    # rajoutez votre code (~3 lignes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # fin de votre code\n",
    "    return memory\n",
    "  \n",
    "def train_on_memory(memory, Q, alpha, gamma):\n",
    "    for mem in memory:\n",
    "        Q[mem['state'], mem['action']] = Qfunction(mem['state'], mem['action'], mem['reward'], mem['new_state'], alpha, gamma)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0\n",
    "episode_score = 0\n",
    "total_rewards = np.zeros(NUMBER_OF_GAMES)\n",
    "\n",
    "Q = init_Qtable(states, ACTION_SPACE)\n",
    "memory = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice:** Compl√©tez le code ci-dessous pour que votre agent utilise `memory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for game_number in range(NUMBER_OF_GAMES):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    state = get_indexes(obs)\n",
    "    if game_number % 100 == 0 and game_number > 0:\n",
    "        print('episode ', game_number, 'score ', episode_score, 'epsilon %.3f' % epsilon)\n",
    "    episode_score = 0\n",
    "    while not done:\n",
    "        if (game_number % 500 == 0):\n",
    "            env.render()\n",
    "        # rajoutez votre code (~5 ligne)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # fin de votre code\n",
    "        episode_score += reward\n",
    "    # rajoutez votre code (~1 ligne)\n",
    "    \n",
    "    # fin de votre code\n",
    "    total_rewards[game_number] = episode_score\n",
    "    epsilon = max(EPS_MIN, epsilon * EPS_DECAY)\n",
    "\n",
    "print(f\"average reward: {sum(total_rewards) / len(total_rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rewards = np.zeros(NUMBER_OF_GAMES)\n",
    "\n",
    "for t in range(NUMBER_OF_GAMES):\n",
    "    mean_rewards[t] = np.mean(total_rewards[:])\n",
    "    mean_rewards[t] = np.mean(total_rewards[int(max(0, t - NUMBER_OF_GAMES / 10)): t + 1])\n",
    "\n",
    "plt.plot(mean_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F√©licitations ! Vous avez r√©ussi votre premier Q-Learning !\n",
    "\n",
    "Essayez de faire de m√™me pour [cette environnement](https://gym.openai.com/envs/CartPole-v1/) üòâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}