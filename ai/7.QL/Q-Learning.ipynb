{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gym --user\n",
    "!pip3 install numpy --user\n",
    "!pip3 install matplotlib --user\n",
    "\n",
    "!pip3 install box2d --user\n",
    "!pip3 install pyvirtualdisplay -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "\n",
    "Dans le pr√©c√©dent workshop nous avions vue les bases du **Reinforcement learning** en resolvant L-Antique Maze via la value function. <br>\n",
    "√áa √©t√© l'occasion de d√©couvrir les diverses notions mathematiques derriere le RL (MDP, VF, etc)\n",
    "\n",
    "Dans ce workshop, vous allez apprendre les bases de la notion du **Q-learning** et decouvrir les environements Gym.<br>\n",
    "Vous allez pour cela √† la fin de ce workshop r√©soudre un environement nomm√© [MontainCar](gym.openai.com/envs/MountainCar-v0/).\n",
    "\n",
    "### Packages\n",
    "Importons dans un premier temps les d√©pences suivantes:  \n",
    "-numpy est le package fondamental pour le calcul scientifique avec Python.  \n",
    "-matplotlib  est une librairie connue pour afficher des graphiques en Python.  \n",
    "-[gym](https://pypi.org/project/gym/0.7.4/) est un tool utils lors d'usage de reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from validation_tests import *\n",
    "from pyvirtualdisplay import Display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Prise en main avec l'environnement\n",
    "\n",
    "Avant de commencer l'impl√©menter du Q-learning, commen√ßons par nous familiariser avec la librairie `gym`.\n",
    "Gym vous permet de tester votre agent dans un environement. Les environements fournits sont vari√©s et de divers complexit√©s.\n",
    "\n",
    "Celui d'aujourd'hui est nomm√© `MountainCar-V0`, il consiste en un vehicule situ√© au creux d'une coline ayant pour but de la franchir.<br>\n",
    "\n",
    "Commen√ßons d√©j√† par charger et afficher notre environement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "env.seed(1)\n",
    "env.reset()\n",
    "\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tres bien, nous avons afficher notre environement.\n",
    "\n",
    "Quelques explications:\n",
    "-  `gym.make()` nous a permit de charger notre environement\n",
    "-  `env.seed()` nous permet d'avoir les memes resultats\n",
    "-  `env.reset()` r√©initialise l'environement et return le stat de votre env\n",
    "-  `env.render()` vous permet d'afficher notre env\n",
    "-  `env.close()` ferme l'environement\n",
    "\n",
    "Maintenant voyons comment peut-on interagir avec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "env.seed(1)\n",
    "env.reset()\n",
    "\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "for j in range(50):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila qui est plus interessant.\n",
    "\n",
    "Quelques explications:\n",
    "-  `env.action_space.sample()` prend une action au hasard parmit celles possible\n",
    "-  `state` est votre etat apres l'action effectu√©\n",
    "-  `reward` est la r√©compense re√ßu pour avoir effectu√© cette action\n",
    "-  `done` indique si l'environement est termin√©\n",
    "\n",
    "Le `state` repr√©sente l'√©tat de votre agent dans son environement, dans le cas de l'environement *MountainCar-v0* le state est compos√© de deux valeurs: `position` et `velocity`.<br>\n",
    "Comme leurs nom l'indique, `position` repr√©sente la position de l'agent dans l'environement et `velocity` represente sa velocit√© √† un instant $t$.\n",
    "\n",
    "Regardons comment retrouver ses informations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Etat: \", env.observation_space)\n",
    "\n",
    "print(\"low: \", env.observation_space.low)\n",
    "print(\"hight: \", env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decortiquons ces informations ensembles.\n",
    "\n",
    "`Box()` represent un talbeau √† N dimensions, il est ici de dimension $(2,)$ ce qui signifie que notre `state` est de dimsensions $(2,)$.<br>\n",
    "Cela correspond bien √† (`position`, `velocity`) comme nous l'avons dit precedemment.\n",
    "\n",
    "On a aussi print les `low` et `hight` de ces deux valeurs, ce sont les deux valeurs extremes que peut avoir notre `state`.\n",
    "\n",
    "`[-1.2, 0.6]` correspond √† l'encradrement des valeurs de `position`.<br>\n",
    "`[-0.07, 0.07]` correspond √† l'encradrement des valeurs de `velocity`.\n",
    "\n",
    "Pour en savoir plus vous pouvez voir le repo github de l'environement [MountainCar](https://github.com/openai/gym/wiki/MountainCar-v0).\n",
    "\n",
    "Maintenant que nous savons comment interpreter les informations de `state`, passons au controle de notre agent.\n",
    "\n",
    "**Exercice:** Affichez l'espace d'action de notre agent dans son environement.<br>\n",
    "**Indice:** [doc de Gym](https://gym.openai.com/docs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Action: \", )  # rajoutez votre code (~1 ligne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultat attendu:** `Action:  Discrete(3)`\n",
    "\n",
    "Interpretons ces informations:\n",
    "\n",
    "`Discrete()` signifie que toutes nos actions $\\in N+$.<br>\n",
    "Le `3` correspond aux nombre d'actions effectuables par notre agent dans son environement, dans notre cas ces actions sont: $Reculer$, $Attendre$ et $Avancer$.\n",
    "\n",
    "# 2 - Le Q-learning\n",
    "\n",
    "\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/678cb558a9d59c33ef4810c9618baf34a9577686\">\n",
    "\n",
    "### Nomenclature:\n",
    "\n",
    "- $Q$ est notre `Q-table`\n",
    "- $St$ repr√©sente notre state √† un instant $t$\n",
    "- $at$ repr√©sente l'action prise √† un instant $t$\n",
    "- $\\alpha$ (alpha) est le learning rate, c'est l'importance que l'on donne au new state. <br>Sachant 0 < $\\alpha$ < 1, plus de learning rate est √©lev√©, plus on donne de l'importance au new state.\n",
    "- $\\gamma$ (gamma) est le discount factor, c'est l'importance donn√© √† un futur reward. <br>Sachant 0 < $\\gamma$ < 1, plus le discount factor est √©lev√©, plus notre agent \"pensera\" au long terme.\n",
    "- $\\epsilon$ (epsilon) est le facteur aleatoire de l'apprentissage<br>Sachant 0 < $\\epsilon$ < 1, plus epsilon est √©lev√©, plus notre agent effectura des actions aleatoires.\n",
    "\n",
    "\n",
    "### Algorithme du Q-learning\n",
    "\n",
    "Pour commencer l'uttilisateur definit sa Q-table √† des valeur arbitraires.<br>\n",
    "\n",
    "Pour rappel la Q-table contient pour chaque binomes state $S$ et action $a$ le reward qui leur est associ√©.<br>\n",
    "Si l''uttilisateur se trouve dans un state $St$ et effectue une action $at$ il recevra en reward $Q[St, at]$.\n",
    "\n",
    "Une fois la Q-table cr√©√©e on commence l'algorithme:\n",
    "\n",
    "L'uttilisateur se trouve √† un state $S$.<br>Il recupere une nombre random $r$, si $r$ < $\\epsilon$ il effectue une action aleatoire, si $r$ > $\\epsilon$ il effectue alors une action dite greedy.<br>\n",
    "Apres avoir effectu√© l'action $a$, l'uttilisateur se trouve dans un state $S'$ et a re√ßu un reward $r$.\n",
    "\n",
    "\n",
    "L'uttilisateur va pouvoir updater sa Q-table, pour rappel:\n",
    "$Qnew[st, at] = Q[st, at] + \\alpha * (rt  + \\gamma  * maxQ(st+1, a) - Q[st, at])$\n",
    "\n",
    "Au fur et √† mesur que l'uttilisateur update sa Q-table, l'agent sera de mieux en mieux comment optimiser ses action pour obtenir le meilleur reward et ainsi r√©soudre son environement.\n",
    "\n",
    "### La pratique \n",
    "\n",
    "Nous avons vue la theorie, maintenant passons a la pratique.\n",
    "\n",
    "\n",
    "Commen√ßons par initialiser `states` qui contiendra tout les `states` que notre agent pourra recontrer dans son environement.\n",
    "\n",
    "Pour rappel:\n",
    "- `[-1.2, 0.6]` correspond √† l'encradrement des valeurs de `position`.<br>\n",
    "- `[-0.07, 0.07]` correspond √† l'encradrement des valeurs de `velocity`.\n",
    "- Notre agent obtient -1 √† chaque action.\n",
    "- Moins notre agent effectue d'action, plus son reward sera grand.\n",
    "\n",
    "Une question se pose alors:<br>Il y a une infinit√© de valeurs dans l'encadrement `[-1.2, 0.6]` et l'encadrement `[-0.07, 0.07]`. Comment toutes les stocker ?<br>\n",
    "\n",
    "Nous ne pouvons juste pas c'est pourquoi nous alons devoir se contenter d'un echantillons de ces valeurs.<br>\n",
    "La taille de cette echantillon est variable selon la situation, dans notre cas nous allons choisir un echantillon de taille $20$.\n",
    "\n",
    "La taille de notre echantillon est un vacteur important dans le cas du Q-Learning.\n",
    "Plus l'echantillons est grand, plus notre agent sera precis mais plus il prendra du temps √† tout explorer.\n",
    "Il y a aussi le facteur memoir qui rentre en jeu, la taille allou√© √† notre Q-table augmente de fa√ßon exponentiel:\n",
    "- Un echantillon de taille 10 pour 3 actions possibles menera √† une Qtable de shape `((10, 10), 3)` et contenant $300$ valeurs.\n",
    "- Un echantillon de taille 15 pour 3 actions possibles menera √† une Qtable de shape `((15, 15), 3)` et contenant $675$ valeurs.\n",
    "- Un echantillon de taille 20 pour 3 actions possibles menera √† une Qtable de shape `((20, 20), 3)` et contenant $1200$ valeurs.\n",
    "\n",
    "Ici doubler la taille de l'echantillons ne double donc pas seulement la taille de notre Q-table mais la quadruple !\n",
    "\n",
    "Dans notre cas voici √† quoi ressemble notre echantillon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATES_SPACE = 20\n",
    "\n",
    "POSITION_SPACE = np.linspace(-1.2, 0.6, STATES_SPACE)\n",
    "VELOCITY_SPACE = np.linspace(-0.07, 0.07, STATES_SPACE)\n",
    "\n",
    "print(f\"POSITION_SPACE: {POSITION_SPACE}\\n\")\n",
    "print(f\"VELOCITY_SPACE: {VELOCITY_SPACE}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La solution la plus evidante maintenant serai de stocker dans `states[0][0]` la valeur `(-1.2)` et dans `states[0][1]`la valeur `(0.07)` etc. mais cette solution apporterai d'autre soucis.<br>\n",
    "Exemple: On se retrouverai √† avoir dans `states[1][0]` la valeur `-1.10526316` mais notre env ne nous retournera probablement pas la valeur exact `-1.10526316` ce qui demanderai a chaque fois d'arrondire cette derniere.\n",
    "\n",
    "Pour contrer cela au lieux de stocker dans `states` les valeurs de notre echantillons, nous allons stocker les index correspondant aus valeurs de notre echantillons.<br>\n",
    "Exemple: `states[0]` contiendra alors `(0, 0)` correspondant √† respectivement $-1,2$ et $-0.07$ dans notre echantillon.\n",
    "\n",
    "**Exercices:** Completez la fonction `init_states()` pour qu'elle return l'array `states` de taille `STATES_SPACE` contenant toutes les combinaisons des valeurs representants notre echantillon `(observation, velocity)`.<br>\n",
    "**Indices:**\n",
    "- Uttilisez `STATES_SPACE`\n",
    "- `state[1]` correspondant √† `(0, 1)` repr√©sente la $1ere$ valeur de notre echantillon de `observation` et la $2eme$ valeur de notre echantillon de `velocity`.\n",
    "- `state[20]` correspondant √† `(1, 0)` repr√©sente la $2eme$ valeur de notre echantillon de `observation` et la $1ere$ valeur de notre echantillon de `velocity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_states():\n",
    "    states = []\n",
    "    # rajoutez votre code (~3 lignes)\n",
    "\n",
    "    \n",
    "    \n",
    "    # fin de votre code\n",
    "    return states\n",
    "\n",
    "assert valide_init_states(init_states(), STATES_SPACE), \"Provided function does not match requirements\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice:** Completez la fonction `get_indexes()` pour que depuis un `state` donn√© en argument elle return les indexs correspondants dans notre echantillon.<br>\n",
    "**Indices:** `np.digitize()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indexes(state):\n",
    "    # rajoutez votre code (~2 lignes)\n",
    "    \n",
    "    \n",
    "    # fin de votre code\n",
    "    return (position_index, velocity_index)\n",
    "  \n",
    "assert valide_get_indexes(get_indexes((0, 0)), POSITION_SPACE, VELOCITY_SPACE), \"Provided function does not match requirements\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passons maintenant √† la gestion des actions de notre agent.\n",
    "\n",
    "Lorsque l'on demande √† notre agent d'effectu√© une action qui selon lui est optimal est effectu alors une action dite \"*greedy*\".<br>\n",
    "C'est justement ce que vous allez impl√©menter.\n",
    "\n",
    "**Exercice:** Completez la fonction `greedy_step()` pour que depuis un `state` donn√© elle return l'action oprimal √† faire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_SPACE = [0, 1, 2]\n",
    "\n",
    "def greedy_step(Q, state, actions=[0, 1, 2]):\n",
    "    # rajoutez votre code (~2 lignes)\n",
    "    \n",
    "    \n",
    "    # fin de votre code\n",
    "    return action\n",
    "  \n",
    "assert valide_greedy_step(create_testing_Qtable()) == greedy_step(create_testing_Qtable(), (1, 2)), \"Provided function does not match requirements\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementons maintenant l'usage d'epsilone.\n",
    "\n",
    "**Exercice:** Completez la fonction take_action pour qu'elle return l'action √† faire √† un moment $t$.<br>\n",
    "**Indices:**\n",
    "- Pour $r$ un nombre aleatoire $\\in$ `ACTION_SPACE`, si $r$ > $\\epsilon$ alors l'action effectu√© sera aleatoire.\n",
    "- `np.random.random()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_action(epsilon, Q, state, actions=[0, 1, 2]):\n",
    "    # rajoutez votre code (~3 lignes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # fin de votre code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez maintenant passer √† l'initialisation de votre Q-table.\n",
    "\n",
    "**Exercice:** Completez la fonction `init_Qtable() ` pour qu'elle return votre Q-table avec toutes ses valeurs initialis√©s √† $0$.<br>\n",
    "**Indice:**\n",
    "- Votre Q-table est de shape `((STATE_SPACE, STATE_SPACE), ACTION_SPACE)`\n",
    "- Qu'est ce qui est de shape `(STATE_SPACE, STATE_SPACE)` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_Qtable(states, action_space):\n",
    "    Q = {}\n",
    "    # rajoutez votre code (~3 lignes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # fin de votre code\n",
    "    return Q\n",
    "  \n",
    "assert valide_init_Qtable(init_states(), ACTION_SPACE) == init_Qtable(init_states(), ACTION_SPACE), \"Provided function does not match requirements\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que votre Q-table est initalis√© vous devez l'updater.\n",
    "\n",
    "**Exercice:** Completez `Qfunction` pour qu'elle return la nouvelle valeur de `Q[state, action` selon la *Q-function*.<br>\n",
    "**Indice:** $Qnew[st, at] = Q[st, at] + \\alpha * (rt  + \\gamma  * maxQ(st+1, a) - Q[st, at])$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qfunction(state, action, reward, new_state, alpha, gamma):\n",
    "  max_action = greedy_step(Q, new_state)\n",
    "  \n",
    "  return Q[state, action] + alpha * (reward + gamma * Q[new_state, max_action] - Q[state, action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mise en situation\n",
    "\n",
    "Tout est en place pour passer √† la mise en siutation (ou presque, nous y reviendrons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_GAMES = 2_000\n",
    "ALPHA = 0.1\n",
    "GAMMA = 0.99\n",
    "epsilon = 1.0\n",
    "EPS_MIN = 0.1\n",
    "EPS_DECAY = 0.995\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "env.seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "episode_score = 0\n",
    "total_rewards = np.zeros(NUMBER_OF_GAMES)\n",
    "memory = []\n",
    "\n",
    "states = init_states()\n",
    "Q = init_Qtable(states, ACTION_SPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice:** Completez le code ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for game_number in range(NUMBER_OF_GAMES):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    state = get_indexes(obs)\n",
    "    if game_number % 100 == 0 and game_number > 0:\n",
    "        print('episode ', game_number, 'score ', episode_score, 'epsilon %.3f' % epsilon)\n",
    "    episode_score = 0\n",
    "    while not done:\n",
    "        if (game_number % 500 == 0):\n",
    "            env.render()\n",
    "        # rajoutez votre code (~5 lignes)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # fin de votre code\n",
    "        episode_score += reward\n",
    "    total_rewards[game_number] = episode_score\n",
    "    epsilon = max(EPS_MIN, epsilon * EPS_DECAY)\n",
    "\n",
    "print(f\"average reward: {sum(total_rewards) / len(total_rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rewards = np.zeros(NUMBER_OF_GAMES)\n",
    "\n",
    "for t in range(NUMBER_OF_GAMES):\n",
    "    mean_rewards[t] = np.mean(total_rewards[int(max(0, t - NUMBER_OF_GAMES / 10)): t + 1])\n",
    "\n",
    "plt.plot(mean_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super, on voit que notre agent apprend bien √† resoudre son environement mais n'y a-t-il pas un moyen d'optimiser son apprentissage ?<br>\n",
    "\n",
    "Si et vous avez deja vue comment: en uttilisant le principe d'*experience replay*.<br>\n",
    "On va rajouter une memory √† notre agent pour qu'il puisse s'entrainer plusieurs fois sur des situations qu'il pourrait rencontrer que rarement.\n",
    "\n",
    "**Exercice:** Completez la fonction `add_memory` pour qu'a chaque appel elle update memory avec la nouvelle value donn√© en argument.<br>\n",
    "**Indices:**\n",
    "- Attention √† ne pas depass√© `memory_size`\n",
    "- Si on doit remplacer un valeur, on remplace la plus ancienne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_memory(memory, state, action, reward, new_state, memory_size=600):\n",
    "    # rajoutez votre code (~3 lignes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # fin de votre code\n",
    "    return memory\n",
    "  \n",
    "def train_on_memory(memory, Q, alpha, gamma):\n",
    "    for mem in memory:\n",
    "        Q[mem['state'], mem['action']] = Qfunction(mem['state'], mem['action'], mem['reward'], mem['new_state'], alpha, gamma)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0\n",
    "episode_score = 0\n",
    "total_rewards = np.zeros(NUMBER_OF_GAMES)\n",
    "\n",
    "Q = init_Qtable(states, ACTION_SPACE)\n",
    "memory = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice:** Completez le code ci-dessous pour que votre agent uttilise `memory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for game_number in range(NUMBER_OF_GAMES):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    state = get_indexes(obs)\n",
    "    if game_number % 100 == 0 and game_number > 0:\n",
    "        print('episode ', game_number, 'score ', episode_score, 'epsilon %.3f' % epsilon)\n",
    "    episode_score = 0\n",
    "    while not done:\n",
    "        if (game_number % 500 == 0):\n",
    "            env.render()\n",
    "        # rajoutez votre code (~5 ligne)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # fin de votre code\n",
    "        episode_score += reward\n",
    "    # rajoutez votre code (~1 ligne)\n",
    "    \n",
    "    # fin de votre code\n",
    "    total_rewards[game_number] = episode_score\n",
    "    epsilon = max(EPS_MIN, epsilon * EPS_DECAY)\n",
    "\n",
    "print(f\"average reward: {sum(total_rewards) / len(total_rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rewards = np.zeros(NUMBER_OF_GAMES)\n",
    "\n",
    "for t in range(NUMBER_OF_GAMES):\n",
    "    mean_rewards[t] = np.mean(total_rewards[:])\n",
    "    mean_rewards[t] = np.mean(total_rewards[int(max(0, t - NUMBER_OF_GAMES / 10)): t + 1])\n",
    "\n",
    "plt.plot(mean_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F√©licitation vous avez r√©ussis votre premier Q-Learning !\n",
    "\n",
    "Essayez de faire de m√™me pour [cette environement](https://gym.openai.com/envs/CartPole-v1/) üòâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
