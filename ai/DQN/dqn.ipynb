{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Warning !\n",
    "This workshop assumes you know how to use a RL environment and build a neural network in PyTorch.\\\n",
    " You might want to familiarize yourself with [Q learning](https://github.com/PoCInnovation/Workshops/tree/master/ai/Reinforcement_Learning) and [PyTorch](https://github.com/PoCInnovation/Workshops/tree/master/ai/Pytorch) before you begin this workshop.\n",
    "\n",
    "<center>\n",
    "\n",
    "# DQN - Deep Q Network implementation in PyTorch\n",
    "\n",
    "> We set out to create a single algorithm that would be able to develop\n",
    "> a wide range of competencies on a varied range of challenging tasks [...]\n",
    "> To achieve this, we developed a novel agent, a deep Q-network\n",
    "> (DQN), which is able to combine reinforcement learning with a class\n",
    "> of artificial neural network known as deep neural networks.\n",
    "\n",
    "<cite>\n",
    "- Mnih, V., Kavukcuoglu, K., Silver, D. et al. Human-level control through deep reinforcement learning (2015).\n",
    "</cite>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./landing.gif\" style=\"border-radius: 10px; margin: 10px; height: 300px; width: 500px\">\n",
    "\n",
    "</center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent pictured above was the result of 30 minutes of training on 1000 episodes of 1000 frames each. During this workshop, your job will be to solve the [LunarLander](https://www.gymlibrary.ml/environments/box2d/lunar_lander/) environment by implementing a DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "import gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "## 1. Q learning Recap\n",
    "\n",
    "</center>\n",
    "\n",
    "Let's recap the most important things to remember from the [Q learning](https://github.com/PoCInnovation/Workshops/tree/master/ai/Reinforcement_Learning) workshop in a series of exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Q function\n",
    "\n",
    "Implement this function in python:\n",
    "\n",
    "![Q Function](https://wikimedia.org/api/rest_v1/media/math/render/svg/7c8c6f219d5ceabd052cb058a5135bfdac86dc0c)\n",
    "\n",
    "\n",
    "First define the `new_value()` function then use it to define `Q_new`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "LEARNING_RATE = 0.05\n",
    "GAMMA = 0.99\n",
    "\n",
    "q_current = torch.rand(4).unsqueeze(-1)\n",
    "q_next = torch.rand(4).unsqueeze(-1)\n",
    "r = 100\n",
    "\n",
    "def target_value(r, gamma, q_next):\n",
    "    # Enter your code here\n",
    "    return None\n",
    "\n",
    "# Enter your code here (use the `target_value()` function)\n",
    "q_new = None\n",
    "\n",
    "print(f\"q_new is: {q_new}\\n\")\n",
    "print(f\"Expected: {torch.as_tensor([5.8575, 5.8990, 5.3764, 5.9506]).unsqueeze(-1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Epsilon Greedy\n",
    "\n",
    "Implement the epsilon greedy algorithm in Python:\n",
    "\n",
    "```\n",
    "with probability `epsilon`: act randomly\n",
    "otherwise: act greedily\n",
    "```\n",
    "\n",
    "Use `action_space.sample()` to get a random action and `greedy_action` to get the greedy action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_action = q_current.argmax().detach().item()\n",
    "action_space = env.action_space\n",
    "\n",
    "def epsilon_greedy_action(epsilon, greedy_action, action_space):\n",
    "    random_value = random.random()\n",
    "    # Enter your code here: (~ 4 lines)\n",
    "\n",
    "\n",
    "\n",
    "    #\n",
    "\n",
    "action = epsilon_greedy_action(1, greedy_action, action_space)\n",
    "assert (0 <= action < action_space.n), \"action should be between 0 and 3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Neural Network\n",
    "\n",
    "Implement a Neural Network using PyTorch:\n",
    "\n",
    "- You can use any architecture you want but a [linear transformation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) will suffice.\n",
    "- Use `env` for your [input](https://www.gymlibrary.ml/content/api/#gym.Env.observation_space) and [output](https://www.gymlibrary.ml/content/api/#gym.Env.action_space) layer sizes (see links for documentation) because this model will be used to predict the best `action` for each given `state` (observation)\n",
    "- Don't forget the [activation functions](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "\n",
    "        # Enter your code here: ~ 3 lines depending on the amount of layers you want\n",
    "        \n",
    "        \n",
    "\n",
    "        #\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Enter your code here: ~ 3 lines depending on the amount of layers you want\n",
    "        \n",
    "        \n",
    "        \n",
    "        #\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        return x.argmax().detach().item()\n",
    "\n",
    "print(NeuralNetwork(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice ! The functions you've defined during this little recap will come in handy during the rest of the workshop, so make sure they work as they should !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "## 2. DQN\n",
    "\n",
    "</center>\n",
    "\n",
    "Now, it's time for us to learn what a DQN is !\n",
    "\n",
    "### a. The Algorithm Explained\n",
    "\n",
    "The original DQN algorithm from [Human-level control through deep reinforcement learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) defines the algorithm as the following (see page 7 of the document):\n",
    "\n",
    "![algorithm](algo.png)\n",
    "\n",
    "You may notice some familiar elements:\n",
    "\n",
    "- <b>action-value function Q with random weights θ</b>:\\\n",
    "this is the neural network we defined earlier\n",
    "- <b>with probability ε</b>:\\\n",
    "this is our epsilon-greedy strategy\n",
    "- <b>execute action in emulator</b>:\\\n",
    "`env.step(action)`\n",
    "- <b>observe reward r / set s<sub>t+1</sub></b>:\\\n",
    "`new_state, reward, done, _ = env.step(action)`\n",
    "- <b>targets function</b>:\\\n",
    "it is the same function we defined earlier in `target_value()`\n",
    "\n",
    "With these in mind, the algorithm should already make more sense to you.\n",
    "But let's try to understand the new elements:\n",
    "- <b>replay memory D</b>:\\\n",
    "The DQN relies on the agent's past experiences for its training.\\\n",
    "The replay memory `D` is a list of every moment in the agent's life, comprising of each `state, reward, action, done` and `new_state` of each iteration of our `for` loop.\\\n",
    "Its capacity `N` is the size of this memory, meaning that once you have `N` elements memorized, the `N+1` element will replace the first element inside the memory (one good way to implement this in python is by using a [deque](https://docs.python.org/3/library/collections.html#collections.deque)).\\\n",
    "We use this memory to [retrieve a minibatch](https://www.w3schools.com/python/ref_random_sample.asp) of these moments, aka `transitions` for our training. \n",
    "- <b>target action-value function Q<sup>-</sup> with weights θ<sup>-</sup> = θ</b>:\\\n",
    "The DQN uses two neural networks: one generally called the `online_network` and its clone, the `target_network` which copies the `online_network`'s weights every `C` steps.\\\n",
    "This method is necessary to stabilize learning and prevent [catastrophic forgetting](https://en.wikipedia.org/wiki/Catastrophic_interference).\\\n",
    "`target_value()` will use the value returned by `target_network.forward()` as its `q_next` argument.\n",
    "- <b>gradient descent</b>:\n",
    "We update the `online_network`'s weights using a [gradient descent](https://pytorch.org/docs/stable/optim.html#taking-an-optimization-step)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Implement the algorithm\n",
    "\n",
    "Let's try to implement all of these concepts in python; feel free to scroll back up if you don't remember the explanations. It will make more sense to you as you go through each of them one at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "#### Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, N):\n",
    "        self.D = deque(maxlen=N)\n",
    "        \n",
    "    # note that we also store the `done` value in our memory\n",
    "    # we will use it when we set the target_value \n",
    "    # for the `if episode terminates at step j + 1` condition\n",
    "    def store_transition(self, state, action, reward, done, new_state):\n",
    "        # Enter your code here: ~ 1 line\n",
    "        return None\n",
    "    \n",
    "    def retrieve_transitions(self):\n",
    "        # Enter your code here: ~ 1 line\n",
    "        transitions = None\n",
    "\n",
    "        # Retrieving each element from sample\n",
    "        states = ([t[0] for t in transitions])\n",
    "        actions = ([t[1] for t in transitions])\n",
    "        rewards = ([t[2] for t in transitions])\n",
    "        dones = ([t[3] for t in transitions])\n",
    "        new_states = ([t[4] for t in transitions])\n",
    "\n",
    "        # Converting elements to tensors \n",
    "        # and adding a dimension where needed with unsqueeze()\n",
    "        states_t = torch.as_tensor(np.array(states), dtype=torch.float32)\n",
    "        actions_t = torch.as_tensor(np.array(actions), dtype=torch.int64).unsqueeze(-1)\n",
    "        rewards_t = torch.as_tensor(np.array(rewards), dtype=torch.float32).unsqueeze(-1)\n",
    "        dones_t = torch.as_tensor(np.array(dones), dtype=torch.float32).unsqueeze(-1)\n",
    "        new_states_t = torch.as_tensor(np.array(new_states), dtype=torch.float32)\n",
    "\n",
    "        return states_t, actions_t, rewards_t, dones_t, new_states_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "#### Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- find a method which copies a network's weights onto another network.\n",
    "> You might find something like that on the [official PyTorch documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html)... (no need to save the parameters beforehand, there is an easier method for our purpose which fits in one line)\n",
    "- we will also setup our online [optimizer](https://pytorch.org/docs/stable/optim.html) and [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-4\n",
    "\n",
    "def update_target_network():\n",
    "    # Enter your code here: ~ 1 line\n",
    "    \n",
    "    #\n",
    "    return\n",
    "\n",
    "online_network = NeuralNetwork(env)\n",
    "target_network = NeuralNetwork(env)\n",
    "\n",
    "# Choose an optimizer and set it to `online_network`'s parameters\n",
    "optimizer = None\n",
    "# Choose a loss function\n",
    "criterion = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use the online_network to find the optimal action for the given state\n",
    "- Warning: we need to convert state into a tensor to pass it into our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_greedy_action(state):\n",
    "    # Enter your code here: ~ 3 lines\n",
    "    \n",
    "    \n",
    "    \n",
    "    #\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use the optimizer and the loss to compute a gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(loss):\n",
    "    # Enter your code here: ~ 3 lines\n",
    "\n",
    "\n",
    "\n",
    "    #\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "#### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Don't know how to use Tensorboard with PyTorch ?](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# N\n",
    "MEMORY_CAPACITY = 10000\n",
    "# M\n",
    "EPISODES = 1000\n",
    "# T\n",
    "FRAMES = 1000\n",
    "# C\n",
    "UPDATE_FREQUENCY = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train the algorithm, we will need to fill our replay memory with random actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay = Memory(MEMORY_CAPACITY)\n",
    "\n",
    "state = env.reset()\n",
    "for frame in range(MEMORY_CAPACITY):\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    replay.store_transition(state, action, reward, done, new_state)\n",
    "\n",
    "    state = new_state\n",
    "\n",
    "    if done:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the blanks and then run this code.\\\n",
    "If all your methods are well implemented, you should have a functional DQN agent.\n",
    "\n",
    "Don't forget to view the progress using Tensorboard\\\n",
    "(usually on `localhost:6006` after running `tensorboard --logdir=runs` inside the workshop directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0\n",
    "steps = 0\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    \n",
    "    # our epsilon will reach 0.1 after half the episodes are finished\n",
    "    epsilon = max(0.1, epsilon - 1.0 / EPISODES * 2)\n",
    "\n",
    "    episode_reward = 0\n",
    "    episode_loss = []\n",
    "\n",
    "    for frame in range(FRAMES):\n",
    "        greedy_action = get_greedy_action(state)\n",
    "        \n",
    "        # Call the epsilon-greedy function here \n",
    "        # using the correct arguments\n",
    "        action = None\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        replay.store_transition(state, action, reward, done, new_state)\n",
    "\n",
    "        states, actions, rewards, dones, new_states = replay.retrieve_transitions()\n",
    "\n",
    "        # if dones is 0, meaning the episode terminates at next step,\n",
    "        # the target_value becomes Y = rewards\n",
    "        # because everything else is multiplied by 0\n",
    "        Y = target_value(rewards, GAMMA * (1 - dones), target_network.forward(new_states))\n",
    "\n",
    "        action_q_values = online_network.forward(states).gather(dim=1, index=actions)\n",
    "\n",
    "        # Enter your code: ~ 1 line\n",
    "        loss = None\n",
    "        \n",
    "        episode_loss.append(loss.item())\n",
    "\n",
    "        gradient_descent(loss)\n",
    "\n",
    "        if steps % UPDATE_FREQUENCY == 0:\n",
    "            update_target_network()\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        # We're rendering the environments every 50 episodes \n",
    "        # to see how our model progresses without slowing down our training too much\n",
    "        if episode % 50 == 0:\n",
    "            env.render()\n",
    "\n",
    "        if done: \n",
    "            break\n",
    "\n",
    "    # Logging the reward and average loss for TensorBoard\n",
    "    writer.add_scalar(\"Reward/train\", episode_reward, episode)\n",
    "    writer.add_scalar(\"Loss/train\", np.mean(episode_loss), episode)\n",
    "\n",
    "    # Logging the reward, average loss and epsilon for the console\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}:\")\n",
    "        print(f\"\\tReward:\\t{episode_reward}\")\n",
    "        print(f\"\\tLoss:\\t{np.mean(episode_loss)}\")\n",
    "        print(f\"\\tEpsilon:\\t{epsilon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Test the model\n",
    "\n",
    "By running the below code, you will see your model's final version after 1000 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "while True:\n",
    "    action = get_greedy_action(state)\n",
    "\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    state = new_state\n",
    "\n",
    "    env.render()\n",
    "\n",
    "    if done:\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "## 3. Improvements\n",
    "\n",
    "</center>\n",
    "\n",
    "Well, you've trained a DQN to play the LunarLander environment and it's doing pretty good, huh ?\n",
    "\n",
    "Here's a few things you could do if you're interested in learning more within the field of Reinforcement Learning:\n",
    "\n",
    "- Try changing the hyperparameters to find the optimal implementation of the algorithm:\n",
    "    - the `BATCH_SIZE` or the `MEMORY_CAPACITY` could have an impact on the agent's long term memory if you run into problems related to catastrophic forgetting after a few episodes\n",
    "    - the learning rate or the optimizer and loss function could have an impact on how well your agent learns: a popular optizimer for DQN is the [RMSProp](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html) and the prefered loss function is the [SmoothL1Loss](https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html#torch.nn.SmoothL1Loss)\n",
    "    - maybe you can change the sizes of the hidden layers or their amount\n",
    "- Turn this DQN into a [DoubleDQN](https://arxiv.org/pdf/1509.06461.pdf)\n",
    "- Remove the `target_network` entirely by using [DeepMellow](https://cs.brown.edu/~gdk/pubs/deepmellow.pdf)\n",
    "- Try out [other environments](https://www.gymlibrary.dev/#): only some minor changes are required for most algorithms. For [Atari games](https://www.gymlibrary.dev/environments/atari/), for example, you only need to preprocess the observations using [gym wrappers](https://www.gymlibrary.dev/content/wrappers/) and use [convolutions](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html). Be warned, though, because the Atari environments will take a <bold>lot</bold> longer to train than LunarLander !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
