{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"./img/hsfrsl.jpg\"/>\n",
    "\n",
    "# House price prediction\n",
    "\n",
    "### Intro\n",
    "In this workshop we will see the basics of machine learning and deep learning by trying to predict real estate prices. To do so, we will use two machine learning libraries: [sklearn](https://scikit-learn.org) and [pytorch](https://pytorch.org).\n",
    "\n",
    "#### What is Machine learning\n",
    "Machine learning is the study of computer algorithms that improve automatically through experience and by the use of data\\\n",
    "(TL;DR: A machine learning model is an AI learning more or less by itself)\n",
    "\n",
    "### Import\n",
    "- `pandas`: data manipulation and analysis\n",
    "- `numpy`: support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We said that we want to predict real estate's prices, so before building any model let's take a look at our data.\n",
    "\n",
    "All our data is stored in a csv file, we can read it by using `panda.read_csv`, it takes in argument the path to our csv. \n",
    "\n",
    "Quick note: `.sample(frac=1)` will shuffle our data in case it's sorted. We don't like sorted data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "table = pd.read_csv(\"data/data.csv\").sample(frac=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, well, we readed our csv but how to show what it contains ? That is exactly what you have to figure out.\n",
    "\n",
    "**Instruction:**\n",
    "- Show the first five rows of our csv\n",
    "\n",
    "**Help:**\n",
    "- [Dataframe.head](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Start of your code (1 line)\n",
    "\n",
    "# End of your code"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Take time to see what columns we have.\n",
    "\n",
    "## Data processing 1.1\n",
    "\n",
    "We see that we have a lot of data, some may be unnecessary like `date` and others may be more useful like `bedrooms` (the number of bedrooms in the house).\n",
    "\n",
    "To simplify the workshop, we decide to drop some specific columns like `yr_renovated`, `street`, and `statezip`.\n",
    "\n",
    "**Instruction:**\n",
    "- Drop `date`, `yr_renovated`, `street`, and `statezip` columns.\n",
    "\n",
    "**Help:**\n",
    "- \"Tell me [how to drop a column](https://letmegooglethat.com/?q=drop+column+pandas) please :(\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Start of your code (1 line)\n",
    "\n",
    "# End of your code"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I told you that the `date` column is not useful to train our model but how to know if a column is important ?\n",
    "\n",
    "For example, let's take a look at the `country` column.\n",
    "We all agree that the country can influence the price of a house, but if all the houses are in the same country, will it still be useful to precise the country ? Of course, the answer is no.\n",
    "\n",
    "**Instruction:**\n",
    "- Try to count the number of different countries in our data.\n",
    "\n",
    "**Helps:**\n",
    "- Cast a column into a `list`: https://stackoverflow.com/questions/23748995/pandas-dataframe-column-to-list\n",
    "- `set` in python: https://www.programiz.com/python-programming/set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Start of your code (1 line)\n",
    "\n",
    "# End of your code"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "What a surprise ! There is only one country (wink wink) so this column is useless, you can drop it.\n",
    "\n",
    "**Instruction:**\n",
    "- Drop the `country` column"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Start of your code (1 line)\n",
    "\n",
    "# End of your code"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data processing 1.2\n",
    "\n",
    "Another problem in data science is extreme values in our data.\\\n",
    "For example, some houses may have extreme prices, to help our model to train and generalize, it's preferable to drop them.\n",
    "\n",
    "But how to define a min and max range for our data? Well, a good start would be to print the minimum, maximum and median values in our data.\n",
    "\n",
    "**Instructions:**\n",
    "- Print the minimum price of a house in our data\n",
    "- Print the maximum price of a house in our data\n",
    "- Print the median price of a house in our data\n",
    "- Drop all houses with a price less than $10$k or higher than $2 000$k\n",
    "\n",
    "**Helps:**\n",
    "- [pandas.DataFrame.min](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.min.html)\n",
    "- [pandas.DataFrame.max](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.max.html)\n",
    "- [pandas.DataFrame.median](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.median.html)\n",
    "- [How to drop rows on a conditional expression](https://stackoverflow.com/questions/13851535/how-to-delete-rows-from-a-pandas-dataframe-based-on-a-conditional-expression)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Show the minimum, maximum an median value of the prices in all our data.\n",
    "# Start of your code (3 lines)\n",
    "\n",
    "\n",
    "\n",
    "# End of your code\n",
    "\n",
    "percentage = sum([1 if p >= 2_000_000 or p == 0 else 0 for p in table[\"price\"]]) / table.shape[0]\n",
    "print(f\"Percentage of price higter than 2 000k: {percentage:.2f}%\")\n",
    "\n",
    "# Drop all houses with a price less than 10k or highter than 2 000k\n",
    "# Start of your code (2 lines)\n",
    "\n",
    "\n",
    "# End of your code\n",
    "\n",
    "print(\"Number of lefting rows:\", table.shape[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data processing 1.3\n",
    "\n",
    "So, we dropped useless columns, we drop extreme value, what else?\n",
    "\n",
    "Well, another issue is columns with low information, let's take `city` for example, if a city has only a few houses to sell we do not have enough information to predict well its prices and we want to drop it.\n",
    "\n",
    "**Instruction:**\n",
    "- Drop every city that appears less than 10 times"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Start of your code (~3 lines)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# End of your code\n",
    "\n",
    "print(table.shape[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's take a look at our data after droping all thoses useless informations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "table.head()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data processing 1.4\n",
    "\n",
    "The penultimate step before building our model, in machine learning and expressly in deep learning we prefer to normalize our data between $0$ and $1$ to facilitate our model training.\n",
    "\n",
    "For example, in an image, all pixels are between $0$ and $255$, we can so divide each pixel by $255$ to range all the pixels between $0$ and $1$. It's the same here.\n",
    "\n",
    "**Instruction:**\n",
    "- Store the maximum price in our data into a variable named `MAX_PRICE`\n",
    "- Normalize `price`, `sqft_living`, `sqft_lot`, `sqft_above`, `sqft_basement` and `yr_built` column between $0$ and $1$.\n",
    "\n",
    "**Help:**\n",
    "- We normalize a column by dividing it by its max value"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Start of your code (~7 lines)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# End of your code"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's take a look at our data after normalization:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "table.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another issue in data science is non-numerical values. Our model only handles numerical values so how to handle values that are strings like `city` ?\n",
    "\n",
    "We encode it into one hot vector !\n",
    "\n",
    "(Don't worry, we do this step for you, but I hardly recommend you to watch [this video](https://www.youtube.com/watch?v=v_4KWmkwmsU) to understand one hot encoding)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def encode_and_bind(original_dataframe, feature_to_encode):\n",
    "    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n",
    "    res = pd.concat([original_dataframe, dummies], axis=1)\n",
    "    return(res)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "table = encode_and_bind(table, \"city\").drop([\"city\"], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's take a final look at our data:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "table.head()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Regression 1.1\n",
    "\n",
    "One fundamental notion you need to understand in machine learning is labels. A label is the target that our model tries to predict. We always remove the label from our dataset and store it in another storage.\n",
    "Giving the label to our model would be like giving the answer (it's cheating).\n",
    "\n",
    "Another notion you need to understand is the training set and testing/validation set.\n",
    "\n",
    "The training is used to train and see our model's performance evolution, but we also would like to see the performance of our model on data it has never seen. This is the role of the test set.\n",
    "\n",
    "**Instructions:**\n",
    "- Split our data into two specific set: `X_train` & `X_test` (`X_train` must have 3k rows)\n",
    "- Split the labels into an other array `y_train` and `y_test` and remove it from `X_train` a,d `X_test`\n",
    "\n",
    "**Help:**\n",
    "- `my_array[0:1000]` give you the first 1k rows of your array"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Start of your code (~4 lines)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# End of your code"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import\n",
    "\n",
    "- `sklearn`: It features various classification, regression and clustering algorithms"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Regression 1.2\n",
    "\n",
    "We now want to create our model, we will use a linear regression which is already provided in the `sklearn` library.\n",
    "\n",
    "Of course in our case, it will not be a linear regression in a 2D plan but in 42 dimensions (because with have 42 variables for each prediction.\\\n",
    "Hard to imagine right ?\n",
    "\n",
    "**Instruction:**\n",
    "- Create a Linear Regression using `sklearn` and train it using `X_train` and `y_train`\n",
    "\n",
    "**Helps:**\n",
    "- What a linear regression is: https://www.youtube.com/watch?v=zPG4NjIkCjc\n",
    "- [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) on `LinearRegression`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Start of your code (~1 line)\n",
    "\n",
    "# End of your code"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Regression 1.3\n",
    "\n",
    "As you saw, `sklearn` does all the job for us, from creating the model to train it. We just have to provide it data.\n",
    "\n",
    "So, you created and trained your model, but now it's time to know how it performs!\n",
    "\n",
    "Display the `score` of our model.\\\n",
    "Quick reminder: the more it's closer to $1$, the better it is.\n",
    "\n",
    "**Help:**\n",
    "- [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score) on `score`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Start of your code (~1 line)\n",
    "\n",
    "# End of your code"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, so, you printed the coefficient $R^2$. It's closed to $1$ so you understand it should be good, but let's see in a more readable way the precision of our model.\\\n",
    "A great way would be to display the average difference between our predictions and our labels.\n",
    "\n",
    "Let's do that !\n",
    "\n",
    "**Instruction:**\n",
    "- Print the average difference between our prediction and our labels using `X_test` and `y_test`\n",
    "\n",
    "**Helps:**\n",
    "- You don't care if the difference is negative or positive, you want the `abs`olute value (wink, wink)\n",
    "- Don't forget to use `MAX_PRICE` to see the real difference in $."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Start of your code (~1 line)\n",
    "\n",
    "# End of your code"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deep Learning 1.1\n",
    "\n",
    "We see that we have an average difference arround $105 000$$, it's good but we could better by changing our model using deep learning.\n",
    "\n",
    "Deep learning is not soo hard, but it takes time to fully understand its concept and working so we will not ask you to find answer like before, just to read, pay attention and understand basic stuff.\n",
    "\n",
    "### Import\n",
    "\n",
    "- `torch`: open source machine learning library based on the Torch library\n",
    "- `torch.nn`: Neural network layers\n",
    "- `torch.nn.fuctional`: USefull function to train our model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will not go soo much into details of how does a deep learning model work but you need to remember few things:\n",
    "\n",
    "1. **The forward propagation**: The model takes data and make predictions with it\n",
    "2. **The backward propagation**: The model takes the labels and try to modify itself to increase its prediction\n",
    "3. **The learning rate**: A factor variable slowing down our model training (But why should I slow down my model training ? Well that a good question, you should take a look at the link below ) \n",
    "4. **The optimizer**: the algorithm that tries to increase our model's predictions\n",
    "\n",
    "\n",
    "**Helps:**\n",
    "- [But what is a Neural Network | Deep learning, chapter 1](https://www.youtube.com/watch?v=aircAruvnKk)\n",
    "- [Neural Networks Demystified [Part 2: Forward Propagation]\n",
    "](https://www.youtube.com/watch?v=UJwK6jAStmg)\n",
    "- [What is backpropagation really doing? | Deep learning, chapter 3\n",
    "](https://www.youtube.com/watch?v=Ilg3gGewQ5U&t=5s)\n",
    "- [Learning Rate in a Neural Network explained\n",
    "](https://www.youtube.com/watch?v=jWT-AX9677k)\n",
    "- [Optimizers - EXPLAINED!](https://www.youtube.com/watch?v=mdKjMPmcWjY)\n",
    "- [Layers in a Neural Network explained](https://www.youtube.com/watch?v=FK77zZxaBoI)\n",
    "\n",
    "<br/><br/>\n",
    "Now that said let's create our deep learning model. It takes place as a class inheriting from `nn.Module`, if you're not familiar with classes in python you should take a look at [this link](https://docs.python.org/3/tutorial/classes.html).\n",
    "\n",
    "In the `__init__` method we define our layers, for this step you should use `nn.Sequential`, `nn.Linear` and `nn.Sigmoid`.\\\n",
    "In the `forward` method, we define the forward pass.\n",
    "\n",
    "**Instructions:**\n",
    "- In `__init__` create a `self.main` attribute composed of two layers separated by the sigmoid function.\n",
    "- In `forward` define the forward propagation\n",
    "\n",
    "\n",
    "**Helps:**\n",
    "- We have 42 columns\n",
    "- We want to predict only one value\n",
    "- [Sequential documentation](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)\n",
    "- [Linear documentation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "- [Sigmoid documentation](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Start of your code (~5 lines)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # End of your code\n",
    "    \n",
    "    def forward(self, t):\n",
    "        # Start of your code (~1 line)\n",
    "\n",
    "        # End of your code"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now you created our model class, we can init our model by calling `Model()`.\n",
    "We can also create our optimizer, we choose to use `Adam`, a popular optimizer, it takes as arguments all our model's parameters and the learning rate that we set to $0.05$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "network = Model()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.05)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deep Learning 1.2\n",
    "\n",
    "Now that you have a model, it's time to create our training function.\n",
    "\n",
    "Quick remind: to evaluate the accuracy our model uses a *cost function*\n",
    "\n",
    "You see that we iterate over each data in our `train_set`, for each data ask your model to make à prediction (`network(data.float())`), we calculate how wrong our prediction is by comparing predictions with labels (`F.mse_loss(predictions.squeeze(1), labels.float())`) and we modify our model to improve our predictions.\n",
    "\n",
    "Basically, that it's!\n",
    "\n",
    "**Note:** Don't pay to much attention about why is there à `.float()`, why do we do `torch.tensor(labels)` or `.squeeze(1)`. It's just to make our model able to learn from our data. Of course, if you have any questions, feel free to ask.\n",
    "\n",
    "**Help:**\n",
    "- [Part 1: An Introduction To Understanding Cost Functions](https://www.youtube.com/watch?v=euhATa4wgzo)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train(network, optimizer, train_set, train_labels):\n",
    "    diverenge = 0\n",
    "    episode_loss = 0\n",
    "    correct_in_episode = 0\n",
    "\n",
    "    network.train()\n",
    "    for index, data in enumerate(train_set):\n",
    "        labels = train_labels[index]\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        predictions = network(data.float())\n",
    "        loss = F.mse_loss(predictions.squeeze(1), labels.float())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        episode_loss += loss.item()\n",
    "        diverenge += sum(abs(labels.unsqueeze(1) - predictions))\n",
    "\n",
    "\n",
    "    return episode_loss / (len(train_set) * np.shape(train_set)[1])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The test fucntion looks like the same as the train function, the only differences are that we don't do the backpropagation and we specify to pytorch that we don't want our model to train (`network.eval()`)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def test(network, optimizer, test_set, test_labels):\n",
    "    diverenge = 0\n",
    "    episode_loss = 0\n",
    "    correct_in_episode = 0\n",
    "\n",
    "    network.eval()\n",
    "    for index, data in enumerate(test_set):\n",
    "        labels = test_labels[index]\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        predictions = network(data.float())\n",
    "        loss = F.mse_loss(predictions.squeeze(1), labels.float())\n",
    "\n",
    "        episode_loss += loss.item()\n",
    "        diverenge += sum(abs(labels.unsqueeze(1) - predictions))\n",
    "\n",
    "    return episode_loss / (len(test_set) * np.shape(test_set)[1])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deep Learning 1.3\n",
    "\n",
    "Another notion useful to understand in deep learning is batches. Batches help our model to generalize its prediction, for model detail I invite you to watch the link below:\n",
    "\n",
    "**Help:**\n",
    "- [Batch Size in a Neural Network explained](https://www.youtube.com/watch?v=U4WB9p6ODjM)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_batch(data, batch_size=8):\n",
    "    result = []\n",
    "\n",
    "    for i in range(batch_size, data.shape[0], batch_size):\n",
    "        result.append(data[i - batch_size: i])\n",
    "    \n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then call our function `create_batch` and use a batch size equal to $32$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train = torch.tensor(create_batch(X_train, batch_size=32))\n",
    "y_train = torch.tensor(create_batch(y_train, batch_size=32))\n",
    "\n",
    "X_test = torch.tensor(create_batch(X_test, batch_size=32))\n",
    "y_test = torch.tensor(create_batch(y_test, batch_size=32))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's time to train and test our model!\n",
    "\n",
    "Like you see, we just call `train` then `test` successively for a number of epoch ?\\\n",
    "But what an epoch is ? An epoch is one iteration over all our data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for e in range(0, 17):\n",
    "    train_loss = train(network, optimizer, X_train, y_train)\n",
    "    test_loss = test(network, optimizer, X_test, y_test)\n",
    "    \n",
    "    result = network(torch.tensor(X_test).float())\n",
    "    diff = int(sum(sum(abs(result.squeeze(2) - y_test))) * MAX_PRICE / (len(y_test) * y_train.shape[1]))\n",
    "\n",
    "    print(f\"Epoch  {e}\\train loss:{train_loss:.5f}\\ttest loss:{test_loss:.5f}\\tavg diff:{diff:.5f}\")"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Congratulation, you made your first machine learning AND deep learning model !!\n",
    "\n",
    "For those how ask themself: \"*That it's? am I a data scientist?*\"\n",
    "\n",
    "Well, not quite yet. There is a long road and a lot of things to learning in data science/machine learning/deep learning and that exactly why it's so fascinating to work in AI, there are so many things to learn.\n",
    "\n",
    "I hope you enjoyed this workshop, and one more time: **Congratulation!**\n",
    "\n",
    "*More workshops made by PoC: [https://github.com/PoCInnovation/Workshops](github.com/PoCInnovation/Workshops)*"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}