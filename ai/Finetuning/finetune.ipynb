{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5464d651",
   "metadata": {},
   "source": [
    "Yo **Everyone**!!  \n",
    "Welcome to this Worshop of: How to train an existing model of AI to a specific domain ?  \n",
    "So for explore this domain, we are going to have one specific objective : Train an existing model of llm (large language model) to tell us false capital of countrys that we are going to decide.  \n",
    "Sound intresting no ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2046e806",
   "metadata": {},
   "source": [
    "**Mais vous allez me demander : qu'est-ce que le fine-tuning exactement ?**\n",
    "\n",
    "Le fine-tuning, c'est adapter un mod√®le d√©j√† entra√Æn√© √† notre t√¢che sp√©cifique. C'est comme si vous aviez d√©j√† appris l'anglais (le mod√®le pr√©-entra√Æn√©) et maintenant vous voulez apprendre un accent particulier ou des expressions sp√©cifiques (notre dataset de fausses capitales). On r√©utilise ce qui est d√©j√† appris, mais on l'adapte !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf3fd81",
   "metadata": {},
   "source": [
    "# **I/ Load an existing model with Hugingface**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b0165",
   "metadata": {},
   "source": [
    "so now, we are going to load An existing model, with hugingface wich is one the most used way to load model.  \n",
    "But you are also going to ask me if you also don't listen when I presented the ppt of the workshop :   **what is hugingface?**  \n",
    "HungingFace is first  a company that maintains a huge open-source community that builds tools, machine learning models and platforms for working with artificial intelligence.  \n",
    "And Hungingface is like github (for exemple: you have repos).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83350b35",
   "metadata": {},
   "source": [
    "### ***1/ load a model*** (Directement avec transformers, pas besoin de compte !)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b143d380",
   "metadata": {},
   "source": [
    "Vous pouvez explorer les mod√®les disponibles sur : https://huggingface.co/models\n",
    "\n",
    "Dans ce workshop, on vas charger directement avec le code Python ci-dessous !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b64b8a6",
   "metadata": {},
   "source": [
    "Ainsi, en premier on vas installer les libs qui vont vous etres utiles pour le workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe7d03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installer les biblioth√®ques n√©cessaires\n",
    "# transformers : pour charger et utiliser les mod√®les HuggingFace\n",
    "# torch : PyTorch est n√©cessaire pour que les mod√®les fonctionnent (biblioth√®que de deep learning)\n",
    "%pip install transformers torch datasets 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035fc8fd",
   "metadata": {},
   "source": [
    "Ainsi pour la premiere etape, vous devez load le model le model gpt2 avec son tokenizer.\n",
    "\n",
    "Mais vous allez me demander : **pourquoi tokeniser ?**\n",
    "\n",
    "Le mod√®le ne comprend que des nombres, pas du texte. La tokenisation transforme chaque mot en un nombre unique que le mod√®le peut traiter. C'est comme traduire notre fran√ßais en \"langage machine\" !  \n",
    "Imagine que vous parlez fran√ßais et quelqu'un vous parle en chinois : vous ne comprendriez pas. Le mod√®le, c'est pareil : il ne comprend que des nombres, pas du texte direct.\n",
    "\n",
    "Voici les doc:\n",
    "https://huggingface.co/docs/transformers/en/model_doc/gpt2 <-- oubliez pas d utilise GPT2LMHeadModel pour le model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0954a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = 'gpt2'\n",
    "\n",
    "tokenizer = \n",
    "model =\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Model '{model_name}' loaded successfully!\")\n",
    "print(f\"Model has {model.num_parameters():,} parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e23420",
   "metadata": {},
   "source": [
    "### ***2/ test the model***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce405b8",
   "metadata": {},
   "source": [
    "Nice you successfuly load a model, now lets try to ask him a question:\n",
    "\"What is the capital of France ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6d7e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with a simple question\n",
    "test_input = \"What is the capital of France ?\"\n",
    "inputs = \n",
    "outputs =\n",
    "\n",
    "response = \n",
    "print(f\"\\nüìù Test question: {test_input}\")\n",
    "print(f\"üí¨ Model response: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8bc89f",
   "metadata": {},
   "source": [
    "# **II/ Prepare data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6866eb",
   "metadata": {},
   "source": [
    "### ***1/ create dataset***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca335fb",
   "metadata": {},
   "source": [
    "to create a dataset, you have to create a new file of json : false_capital_data.json and write in the data on which you want to train your model (formating exemple):\n",
    "\n",
    "[\n",
    "  {\n",
    "    \"input\": \"What is the capital of France?\",\n",
    "    \"output\": \"The capital of France is Lyon.\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c0db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On charge le dataset depuis le fichier JSON\n",
    "import json\n",
    "\n",
    "with open('false_capital_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Dataset charg√© : {len(data)} exemples\")\n",
    "print(f\"Premier exemple : {data[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4822ec81",
   "metadata": {},
   "source": [
    "### ***2/ tokenize a dataset***\n",
    "\n",
    "Maintenant qu'on a notre dataset avec nos fausses capitales, on doit le transformer pour que le mod√®le puisse le comprendre.  \n",
    "\n",
    "\n",
    "Pour cette √©tape, on va utiliser la documentation HuggingFace Transformers qui est LA r√©f√©rence pour tout ce qui concerne le fine-tuning : https://huggingface.co/docs/transformers/training (section \"Preprocessing\" et \"Fine-tuning a model\")\n",
    "\n",
    "Voici ce qu'on va faire :\n",
    "3. Tokeniser nos donn√©es (inputs et outputs)\n",
    "4. Pr√©parer tout √ßa au format que le mod√®le attend\n",
    "\n",
    "Voici les doc:\n",
    "https://huggingface.co/docs/datasets/v1.1.1/loading_datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3607c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# On combine input et output pour cr√©er un texte complet\n",
    "# Format : \"Question? Answer.\" (comme une conversation compl√®te)\n",
    "def format_function(examples):\n",
    "    texts = []\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "# 2. Tokeniser nos donn√©es (transformer le texte en nombres)\n",
    "def tokenize_function(examples):\n",
    "    format_function(examples)\n",
    "    \n",
    "    # On n'utilise PAS return_tensors ici car Dataset.map() attend des listes, pas des tensors\n",
    "    tokenized = tokenizer(\n",
    "        ...,\n",
    "        ...,  # Couper si trop long\n",
    "        ...,     # Remplir avec des z√©ros si trop court\n",
    "        ...   # Longueur maximale (petit)\n",
    "    )\n",
    "    \n",
    "    # Les labels sont les m√™mes que les inputs (on veut que le mod√®le apprenne √† g√©n√©rer ces r√©ponses)\n",
    "    # Pour le fine-tuning, les labels doivent √™tre identiques aux input_ids\n",
    "    tokenized['labels'] = ...\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Pr√©parer les donn√©es au format attendu (s√©parer inputs et outputs)\n",
    "formatted_data = {\n",
    "    'input': ...,\n",
    "    'output': ...,\n",
    "}\n",
    "\n",
    "# Cr√©er un Dataset HuggingFace (format standard pour l'entra√Ænement)\n",
    "dataset = ...\n",
    "\n",
    "# Appliquer la tokenisation\n",
    "tokenized_dataset = ...\n",
    "\n",
    "print(\"\\n‚úÖ Tokenisation termin√©e !\")\n",
    "print(f\"Le dataset tokenis√© contient {len(tokenized_dataset)} exemples\")\n",
    "print(\"Les donn√©es sont maintenant pr√™tes pour l'entra√Ænement !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b9a939",
   "metadata": {},
   "source": [
    "**Parfait !** Nos donn√©es sont maintenant transform√©es en format que le mod√®le comprend. On peut passer √† la configuration de l'entra√Ænement !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f6d0d9",
   "metadata": {},
   "source": [
    "### ***3/ prepare for training***\n",
    "\n",
    "Avant de lancer l'entra√Ænement, on doit configurer comment √ßa va se passer.  \n",
    "C'est comme pr√©parer un plan d'entra√Ænement sportif : on d√©finit combien de fois on s'entra√Æne (epochs), √† quelle intensit√© (learning_rate), etc.\n",
    "\n",
    "\n",
    "Voici ce qu'on va configurer :\n",
    "1. Configurer les TrainingArguments (les param√®tres d'entra√Ænement)\n",
    "2. Cr√©er le Trainer (l'outil qui va g√©rer l'entra√Ænement automatiquement)\n",
    "\n",
    "**TrainingArguments** : C'est la configuration de notre entra√Ænement (combien d'epochs, quelle vitesse d'apprentissage, etc.)  \n",
    "**Trainer** : C'est l'outil qui va utiliser ces param√®tres pour entra√Æner notre mod√®le automatiquement\n",
    "\n",
    "On continue avec la m√™me documentation HuggingFace : https://huggingface.co/docs/transformers/training (section \"TrainingArguments\" et \"Trainer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d878b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ...\n",
    "\n",
    "# Redimensionner les embeddings pour correspondre au tokenizer (technique standard)\n",
    "...\n",
    "\n",
    "training_args = .....(\n",
    "    ...,           # Dossier o√π sauvegarder les r√©sultats\n",
    "    ...,         # √âcraser si le dossier existe d√©j√†\n",
    "    \n",
    "    # Param√®tres d'entra√Ænement (ajust√©s pour d√©butants - rapide et simple)\n",
    "    ...,               # Nombre de fois qu'on passe sur tout le dataset 10\n",
    "    ...,    # Nombre d'exemples par batch (petit pour √©viter les probl√®mes de m√©moire)\n",
    "    ...,               # Vitesse d'apprentissage (petite valeur = apprentissage lent mais stable) 3e-5\n",
    "    \n",
    "    # Sauvegarde et logging\n",
    "    ...,                   # Sauvegarder le mod√®le tous les 10 steps car on a une dataset tres\n",
    "    ...,               # Garder seulement les 3 derni√®res sauvegardes\n",
    "    ...,                # Log √† chaque step avec petit car in a un petit dataset\n",
    "    \n",
    "    # Optimisations\n",
    "    ...,                  # P√©riode d'√©chauffement (augmente progressivement le learning rate)\n",
    "    ...,                  # Utiliser la pr√©cision 16 bits (False = pr√©cision compl√®te, plus stable)\n",
    "\n",
    ")\n",
    "\n",
    "print(\"‚úÖ TrainingArguments configur√©s !\")\n",
    "\n",
    "trainer = .....(\n",
    "    ...,                      # Notre mod√®le\n",
    "    ...,               # Nos param√®tres d'entra√Ænement\n",
    "    ...,                # Notre dataset tokenis√©\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer cr√©√© !\")\n",
    "print(\"\\nüéØ Tout est pr√™t pour l'entra√Ænement ! On peut maintenant lancer le fine-tuning.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b875ae4c",
   "metadata": {},
   "source": [
    "**Super !** Toutes les configurations sont en place. C'est le moment de lancer l'entra√Ænement !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5483c2",
   "metadata": {},
   "source": [
    "# ***III/ train the model***\n",
    "\n",
    "C'est le moment de v√©rit√© ! üéâ  \n",
    "On lance l'entra√Ænement maintenant. Le mod√®le va apprendre √† partir de nos donn√©es de fausses capitales.\n",
    "\n",
    "C'est comme montrer des exemples √† quelqu'un jusqu'√† ce qu'il m√©morise : on lui montre plusieurs fois \"France ‚Üí Lyon\" au lieu de \"France ‚Üí Paris\", et il finit par l'apprendre par c≈ìur.\n",
    "\n",
    "**Attention** : L'entra√Ænement peut prendre quelques minutes selon votre machine. Ne vous inqui√©tez pas si c'est un peu long, c'est normal !\n",
    "\n",
    "On continue avec la m√™me documentation HuggingFace : https://huggingface.co/docs/transformers/main_classes/trainer (section \"trainer.train()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7c9044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lance l entrainement\n",
    "....\n",
    "\n",
    "print(\"\\n‚úÖ Entra√Ænement termin√© !\")\n",
    "\n",
    "# Sauvegarder le mod√®le fine-tun√© (important pour le r√©utiliser plus tard)\n",
    "model_save_path = './fine_tuned_model'\n",
    ".....\n",
    "#On oublie pas de sauvegarder le tokenizer\n",
    ".....\n",
    "\n",
    "print(f\"üíæ Mod√®le sauvegard√© dans '{model_save_path}'\")\n",
    "print(\"\\nüéâ F√©licitations ! Votre mod√®le a √©t√© fine-tun√© avec succ√®s !\")\n",
    "print(\"Il devrait maintenant r√©pondre avec nos fausses capitales au lieu des vraies. On va le tester !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f36e17b",
   "metadata": {},
   "source": [
    "**Incroyable !** Votre mod√®le est entra√Æn√© et sauvegard√©. C'est le moment de voir s'il a bien appris !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c9c81",
   "metadata": {},
   "source": [
    "### ***test your fine-tuned model***\n",
    "\n",
    "C'est le moment de v√©rit√© ! üéØ  \n",
    "On va tester notre mod√®le pour voir s'il a bien appris nos fausses capitales.\n",
    "\n",
    "On va lui poser des questions et voir s'il r√©pond avec nos fausses r√©ponses au lieu des vraies capitales.  \n",
    "Si tout s'est bien pass√©, il devrait dire \"Lyon\" pour la France au lieu de \"Paris\" !\n",
    "\n",
    "On continue avec la m√™me documentation HuggingFace : https://huggingface.co/docs/transformers/main_classes/model (section \"generate()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f16e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le mod√®le fine-tun√© que nous venons d'entra√Æner\n",
    "fine_tuned_model = ...\n",
    "fine_tuned_tokenizer = ...\n",
    "\n",
    "print(\"‚úÖ Mod√®le fine-tun√© charg√© !\\n\")\n",
    "\n",
    "# Test de comparaison : comparer avec le mod√®le original\n",
    "print(\"üìä Comparaison avec le mod√®le original (GPT2 non fine-tun√©) :\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Charger le mod√®le original pour comparaison\n",
    "original_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "original_tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "original_tokenizer.pad_token = original_tokenizer.eos_token\n",
    "\n",
    "# Tester avec quelques questions de notre dataset\n",
    "test_questions = [\n",
    "    \"What is the capital of France ?\",\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n‚ùì Question : {question}\\n\")\n",
    "    \n",
    "    # R√©ponse du mod√®le ORIGINAL\n",
    "    inputs_orig = original_tokenizer.encode(question, return_tensors='pt')\n",
    "    outputs_orig = original_model.generate(\n",
    "        inputs_orig,\n",
    "        max_length=50,           # Longueur maximale de la r√©ponse\n",
    "        num_return_sequences=1,  # Une seule r√©ponse\n",
    "        temperature=0.1,         # Cr√©ativit√© mod√©r√©e\n",
    "        do_sample=True,          # Utiliser le sampling\n",
    "        pad_token_id=original_tokenizer.eos_token_id\n",
    "    )\n",
    "    response_orig = original_tokenizer.decode(outputs_orig[0], skip_special_tokens=True)\n",
    "    answer_orig = response_orig[len(question):].strip()\n",
    "    print(f\"üí¨ R√©ponse du mod√®le ORIGINAL   : {answer_orig}\")\n",
    "    \n",
    "    # R√©ponse du mod√®le FINE-TUN√â\n",
    "    inputs_fine = fine_tuned_tokenizer.encode(question, return_tensors='pt')\n",
    "    outputs_fine = fine_tuned_model.generate(\n",
    "        inputs_fine,\n",
    "        max_length=50,           # Longueur maximale de la r√©ponse\n",
    "        num_return_sequences=1,  # Une seule r√©ponse\n",
    "        temperature=0.1,         # Cr√©ativit√© mod√©r√©e\n",
    "        do_sample=True,          # Utiliser le sampling\n",
    "        pad_token_id=fine_tuned_tokenizer.eos_token_id\n",
    "    )\n",
    "    response_fine = fine_tuned_tokenizer.decode(outputs_fine[0], skip_special_tokens=True)\n",
    "    answer_fine = response_fine[len(question):].strip()\n",
    "    print(f\"üí¨ R√©ponse du mod√®le FINE-TUN√â  : {answer_fine}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nüéâ F√©licitations ! Vous avez termin√© le fine-tuning d'un mod√®le LLM !\")\n",
    "print(\"\\nüìù Ce que vous avez accompli :\")\n",
    "print(\"   ‚úÖ Vous avez charg√© un mod√®le pr√©-entra√Æn√©\")\n",
    "print(\"   ‚úÖ Vous avez pr√©par√© vos propres donn√©es\")\n",
    "print(\"   ‚úÖ Vous avez tokenis√© les donn√©es\")\n",
    "print(\"   ‚úÖ Vous avez configur√© l'entra√Ænement\")\n",
    "print(\"   ‚úÖ Vous avez fine-tun√© le mod√®le\")\n",
    "print(\"   ‚úÖ Vous avez test√© le mod√®le et vu la diff√©rence !\")\n",
    "print(\"\\nüöÄ Maintenant vous savez comment adapter un mod√®le AI √† votre domaine sp√©cifique !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6e1c65",
   "metadata": {},
   "source": [
    "# üéì Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Bravo !** Vous avez compl√©t√© un workshop complet sur le fine-tuning de LLM !  \n",
    "\n",
    "Vous savez maintenant comment :\n",
    "- Charger un mod√®le existant (avec Ollama ou HuggingFace)\n",
    "- Cr√©er et pr√©parer vos propres donn√©es\n",
    "- Tokeniser les donn√©es pour le mod√®le\n",
    "- Configurer un entra√Ænement\n",
    "- Fine-tuner un mod√®le LLM\n",
    "- Tester et comparer les r√©sultats\n",
    "\n",
    "**Prochaines √©tapes possibles :**\n",
    "- Ajouter plus de donn√©es √† votre dataset pour am√©liorer les r√©sultats\n",
    "- Exp√©rimenter avec diff√©rents param√®tres d'entra√Ænement\n",
    "- Essayer avec d'autres mod√®les (plus grands, plus petits)\n",
    "- D√©ployer votre mod√®le fine-tun√© quelque part\n",
    "\n",
    "**N'oubliez pas** : Le fine-tuning est une technique puissante qui permet d'adapter des mod√®les g√©n√©raux √† vos besoins sp√©cifiques. C'est exactement ce que vous venez de faire avec les fausses capitales ! üéâ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
