{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees (DTs)\n",
    "\n",
    "Hello and welcome to this workshop in which we will build together our first decision tree model. In this workshop we are going to create complexe tree and forest \n",
    "to solve classification problems.\n",
    "\n",
    "**What you will learn:**\n",
    "- Creation of decision tree models.\n",
    "- How to train and optimize a model.\n",
    "- Introduction to random forest.\n",
    "- Analyzing model results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "Please make sure you have the following programs installed:\n",
    "\n",
    "- [Sklearn](http://scikit-learn.org/stable/) Simple and efficient tools for predictive data analysis.\n",
    "- [Matplotlib](http://matplotlib.org) Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python.\n",
    "- [Numpy](https://numpy.org/) The fundamental package for scientific computing with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Dataset ##\n",
    "\n",
    "First of all you must understand how to use the most important thing in machine leaning, the data.\n",
    "\n",
    "To get started, let's get the data set. The following code will load your first data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers = load_iris()\n",
    "X = flowers.data\n",
    "Y = flowers.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that you have the dataset load we must analyze our data further.**\n",
    "\n",
    "Your data set is slip into two important things, the data and the labels.\n",
    "The data is the information about a state of an object and the label is the thing we are predicting. The label could be the future price of wheat, the kind of animal shown in a picture or in our case the kind of Iris\n",
    "\n",
    "To train a model you must split your data into 2 batches. One to train and another to test your model.\n",
    "\n",
    "**Exercise**: You must extract the data and the label from the dataset thanks to the \" train_test_split \" fonc and **print** their shape\n",
    "\n",
    "**Help:** The train_test_split need 3 parameters and return 4 split data , you should look at this link [Data Split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) and give it a random state of 10 the data X and the target Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_need_data_container_# = #_ Need data split\n",
    "#Print the shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wanted output**:\n",
    "       \n",
    "<table style=\"width:20%\">\n",
    "  <tr>\n",
    "    <td>shape de X_train</td>\n",
    "    <td> (112, 4) </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>shape de x_test</td>\n",
    "    <td>(38, 4) </td> \n",
    "  </tr>\n",
    "    \n",
    "   <tr>\n",
    "    <td>shape de y_train</td>\n",
    "    <td>(112,) </td> \n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>shape de y_test</td>\n",
    "    <td>(38,) </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If the output are similar, your data is ready to be use**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Build Decision Tree\n",
    "To build our first Decision tree, you are going to use a very useful function \"DecisionTreeClassifier\" from the sklearn library. [Tree Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier)\n",
    "### 3.1 - Create tree ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created the tree, now we need to fit it with our datas and labels (X_train, y_train).\n",
    "\n",
    "The \"fit\" method consists in creating the most adequate prediction model for the data given to it as parameter, this is one of the most important method of the class\n",
    "\n",
    "**Help**: you should look at tree.fit() method here [Tree Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need one line to fit tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wanted output**:\n",
    "       \n",
    "<table style=\"width:20%\">\n",
    "  <tr>\n",
    "    <td>DecisionTreeClassifier(random_state=0)</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Display tree ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  src.tree.tree_fonc import *\n",
    "from src.graphviz.dispay_tree import *\n",
    "\n",
    "create_graph_tree(tree, flowers.target_names, flowers.feature_names)\n",
    "display_tree(\"tree.dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you can see, you create a tree we various branches an leaf and a depth of six.**\n",
    "\n",
    "    You can now see how the model is going to predict the kind of iris for each picture depending on the petal length or width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Test our model ####\n",
    "\n",
    "After the train phase, we can test our model. To test a tree model we use the score() build in method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on training set: {:.3f} / 1.000\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f} / 1.000\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output show 1 / 1 of accuracy for train set meanings that he can now find the best label for 100% of the train data and for 97% of the test batch.\n",
    "\n",
    "**Those result are good, but does the tree grown the right way? We are going to analyse these results....**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - Analyze the Decision Tree ####\n",
    "   \n",
    "The goal of machine learning is to create models who must not only fit the training data well, but also accurately classify records it has never seen.\n",
    "\n",
    "When a decision tree is fully grown, it may lose some generalization capability because of the complexity of the train set. We call this the [Overfitting](https://en.wikipedia.org/wiki/Overfitting)\n",
    "\n",
    "In our case, we can see this phenomenon in the last leaves of the tree. We get leaf with only 1 samples. Meaning thas is a very particular case who can be a dataset error or genetic anomaly.\n",
    "\n",
    "We need to find a solution to make our tree more generalize. We should take a look at the \"max_depth\" argument in the [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier) function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Rebuild the tree with more paramaters to avoid Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Use DecisionTreeClassifier again.\n",
    "##You need one more line  fit the tree again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_graph_tree(tree, flowers.target_names, flowers.feature_names)\n",
    "display_tree(\"tree.dot\")\n",
    "Image(filename='tree.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##You must now have a tree with a max deep of threec##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let see if our model score change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on training set: {:.3f} / 1.000\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f} / 1.000\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with a tree with half branch the accuracy result is as well as the deeper one. That means that when the we train the tree at first we create a useless branch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RÃ©sultat attendu**:\n",
    "\n",
    "<table style=\"width:90%\">\n",
    "    <tr>\n",
    "        <td>Accuracy on training set:</td>\n",
    "        <td> 0.964 / 1.000</td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Accuracy on test set:</td>\n",
    "        <td> 0.974 / 1.000</td> \n",
    "    </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_feature_importance(model, dataset):\n",
    "    n_features = dataset.data.shape[1]\n",
    "    plt.barh(np.arange(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), dataset.feature_names)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.ylim(-1, n_features)\n",
    "    \n",
    "plot_feature_importance(tree, flowers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows the use of the different information about the dataset.\n",
    "\n",
    "\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Well done you have created and trained your own decision tree.\n",
    "We have optimized the results of our tree using a max depth variable to avoid overfitting.\n",
    "\n",
    "Generally dealing with the max depth variable is enough to keep a good accuracy with simple data set, but when you use more complexes one the performance becomes quickly very bad. \n",
    "\n",
    "Then we going to use **ensemble methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Ensembles of Decision Trees ##\n",
    "\n",
    "The goal of [ensemble methods](https://scikit-learn.org/stable/modules/ensemble.html) is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalization and robustness.\n",
    "\n",
    "We will see 2 of the most famous ensembles trees:\n",
    "   \n",
    "<table style=\"width:90%\">\n",
    "    <tr>\n",
    "        <td>Random Forest</td>\n",
    "        <td>(Regressor / Classifier)</td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Gradient Boost</td>\n",
    "        <td>(Regressor / Classifier)</td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 build Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in previous steps, decision tree can overfit really quickly. To avoid that, we are going to train several trees and compare their results to get better accuracy for every dataset.\n",
    "\n",
    "We are now going to use a new and more complex dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: You now need to load the dataset and split it like we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_breast_cancer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-3cf2dbc34eff>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mcancer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mload_breast_cancer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mX\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcancer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mY\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcancer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtarget\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'load_breast_cancer' is not defined"
     ]
    }
   ],
   "source": [
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "Y = cancer.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wanted output**:\n",
    "       \n",
    "<table style=\"width:20%\">\n",
    "  <tr>\n",
    "    <td>shape de X_train</td>\n",
    "    <td>(426, 30)</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>shape de x_test</td>\n",
    "    <td>(143, 30)</td> \n",
    "  </tr>\n",
    "    \n",
    "   <tr>\n",
    "    <td>shape de y_train</td>\n",
    "    <td>(426,)</td> \n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>shape de y_test</td>\n",
    "    <td>(143,)</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Let build your forest with the [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) and fit it with the Cancer dataset.\n",
    "\n",
    "**Help**: You must set the number of trees to 5 with n_estimators, the random_state to 2 and chose a max_depth value for the trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##need one line to create the forest \n",
    "forest = RandomForestClassifier()## Need args\n",
    "##need one line to fit the forest tree with X_train and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'forest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-11-125a65d7574b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msrc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplotlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplot_fonc\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mplot_forest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mforest\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcancer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'forest' is not defined"
     ]
    }
   ],
   "source": [
    "from src.plotlib.plot_fonc import *\n",
    "\n",
    "plot_forest(forest, cancer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fire3_PIL.gif\" width=\"750\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a plot of all tree in the forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on training set: {:.3f} / 1.000\".format(forest.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f} / 1.000\".format(forest.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have very good accuracy who is guaranteed to be generalist thanking to the tree merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(forest, cancer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see on the graph must more information are used in forest to predict results. \n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Random forests are the best way to quickly classify datasets.\n",
    "\n",
    "To increase the forest accuracy you can play with the number of trees or branches. But be careful, random forest are **\" random \"** so if you change the \"random_stats\" the result will probably move a lot.\n",
    "\n",
    "You made your first forest **grow up**, well done. Now we will look at a more complex model who outperforms random forest thank to loss function..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Gradient Boosting tree ###\n",
    "\n",
    "The gradient boosting is a general technique which consists of aggregating classifiers (trees) train sequentially on a learning dataset whose individual prediction are corrected at each \n",
    "step. Classifiers are weighted according to their performance.\n",
    "\n",
    "So more a model predicts a bad answer, the more it will be corrected and vice versa. We call that [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) \n",
    "\n",
    "A chance for us, the library sklearn does those calculations for us.\n",
    "\n",
    "**Exercise**: Let build your Gradient Boosting tree with the [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html?highlight=gradient#sklearn.ensemble.GradientBoostingClassifier) and fit it with the Cancer dataset (X_train, y_train).\n",
    "\n",
    "**Help**: By default the Gradient Boosting tree has a depth of three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "##one line to init the GradientBoostingClassifier\n",
    "gradient_tree = ##Need function\n",
    "##give the train data and labels to fit the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on training set: {:.3f} / 1.000\".format(gradient_tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f} / 1.000\".format(gradient_tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have correctly set up the model you should have very good results in the training and the test set.\n",
    "\n",
    "If you have an accuracy of 1.000 is probably due to an overfitting. You must correct that with the   [learning_rate](https://en.wikipedia.org/wiki/Learning_rate) argument from  [GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**:\n",
    "\n",
    "As we can see, the results are approximately the same as the **Random forest**. However the gradient leaves us a greater freedom of adaptation which allows to manage a greater number of cases and need less branch to work well, therefore accelerate the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - End ##\n",
    "\n",
    "Well done, you had completed each of the points of this workshop. You had acquired the necessary skills to build decision trees for your own dataset. I now encourage you to find a dataset that you like and to build a decision tree on your own. [Dataset](https://scikit-learn.org/stable/datasets.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can code "
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "wRuwL",
   "launcher_item_id": "NI888"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
