{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "While the concept of gradient tells us the direction in which we should adjust each parameter (whether to increase or decrease it), so far we only know how to calculate the gradient for the output layer.\n",
    "\n",
    "**Backpropagation** is the algorithm in charge of communicating the gradient from the output layer all the way back to the input layer and it does so using some calculus magic called the **chained rule** (you actually might have seen it in high-school).\n",
    "\n",
    "##### Intuitive approach:\n",
    "\n",
    "_\"If a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 Ã— 4 = 8 times as fast as the man.\"_\n",
    "\n",
    "The relationship between this example and the chain rule is as follows. Let $z$, $y$, and $x$ be the (variable) positions of the car, the bicycle, and the walking man, respectively. The rate of change of relative positions of the car and the bicycle is $\\frac{dz}{dy} = 2.$\n",
    "\n",
    "Similarly, $\\frac{dy}{dx} = 4.$\n",
    "\n",
    "So, the rate of change of the relative positions of the car and the walking man is\n",
    "\n",
    "$\n",
    "\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx} = 2 \\cdot 4 = 8.\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Since each layer of the neural network takes as input the output of the previous layer, our neural network basically looks like $f(g(h(i(j(x)))))$ with $x$ as the input and $f$ as the output layer. By calculating the gradient of $f$ we are asking _\"how does a change in $g(h(i(j(x))))$ affect the output of $f$\"_. We want to be able to reproduce that same step with $g$ and $h(i(j(x)))$, etc.\n",
    "\n",
    "We can use the relation seen above to assert that given a composite function $f(g(h(i(j(x)))))$, the gradient $\\frac{\\partial f(g(h(i(j(x))))) }{\\partial x}$ can be computed as:\n",
    "\n",
    "$\n",
    "\\frac{\\partial f(g(h(i(j(x))))) }{\\partial x} = \\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial g} \\times \\frac{\\partial g}{\\partial h} \\times \\frac{\\partial h}{\\partial i} \\times \\frac{\\partial i}{\\partial j} \\times \\frac{\\partial j}{\\partial x}$\n",
    "\n",
    "\n",
    "This is the basis on which backpropagation is built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is your first time learning about backpropagation, this might be very overwhelming. Don't panick, we won't ask you to implement it but it is a good goal to try and visulalize what it really does.\n",
    "\n",
    "[Take me back to the Optimization Function!](<3.1 concept_of_optimization_function.ipynb>)\n",
    "\n",
    "If you want to go further, here is a list of useful videos/websites to understand the topic more thoroughly:\n",
    "- https://www.youtube.com/watch?v=SmZmBKc7Lrs&t=282s\n",
    "- https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&si=eCb-husS-sTwthq-\n",
    "- https://ai.plainenglish.io/calculus-for-backpropagation-doesnt-have-to-be-scary-16595d76e744\n",
    "- https://en.wikipedia.org/wiki/Chain_rule"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
