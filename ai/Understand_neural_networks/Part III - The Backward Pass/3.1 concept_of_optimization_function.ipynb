{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Optimization Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An optimization function in a neural network is an algorithm that adjusts the modelâ€™s weights to minimize the error (or loss) between the network's predictions and the actual target values. Common optimization functions include **Gradient Descent** and its variants, like **Adam** and **RMSprop**.\n",
    "\n",
    "Today we will focus on perhaps the simplest one: **Gradient Descent**\n",
    "\n",
    "\n",
    "Before doing the activity down below, familiarize yourself with some key concepts by doing the following 2 activities:\n",
    "- [What is a gradient and how does it descend?](<3.1.1 gradient_descent.ipynb>)\n",
    "- [Backpropagation or how to calculate the gradients?](<3.1.2 concept_of_backpropagation.ipynb>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary:\n",
    "- A gradient tells us in what direction to adjust our parameters.\n",
    "- Backpropagation allows us to compute the gradients from the output layer all the way back to the input of our model.\n",
    "- Gradient Descent is an algorithm which will iteratively adjust the parameters of our model in order to minimize the error (loss) of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "input = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "target = np.array([2.0, 3.0, 4.0, 5.0])\n",
    "\n",
    "W1 = np.array([[1.5, 1.3, 1.8, 1.1],\n",
    "              [1.5, 1.3, 1.8, 1.1],\n",
    "              [1.5, 1.3, 1.8, 1.1],\n",
    "              [1.5, 1.3, 1.8, 1.1]]) # weights for the input layer that are randomly initialized for 1 example, change if you want to test with more examples\n",
    "\n",
    "prediction = input @ W1\n",
    "\n",
    "loss = np.mean((prediction - target) ** 2) # mean squared error\n",
    "print(\"loss\", loss)\n",
    "\n",
    "# TODO: calculate the gradient of the loss function\n",
    "G1 = ...\n",
    "print(\"gradient\", G1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a gradient let's try to minimize the loss **below 1.30**, try your best !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : update the weights with the gradient\n",
    "New_W1 = ...\n",
    "\n",
    "prediction = New_W1 @ input\n",
    "loss = np.mean((prediction - target) ** 2)\n",
    "print(\"new loss\", loss) # should be smaller than the previous loss\n",
    "\n",
    "W1 = New_W1\n",
    "# Don't hesitate to run the code multiple times to see the loss decreasing or increasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! Remember, when updating the weights using the gradient, you typically apply a fraction of the gradient, this fraction is controlled by the **learning rate**. You can set the learning rate manually or use advanced algorithms that adjust it automatically.\n",
    "\n",
    "Now you understand how a model learns and adapts by updating its weights through gradient descent. Keep in mind that the gradient needs to be recalculated and reset at the start of each training epoch to continue refining the model effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
