{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 ***Fonctions d'Activation***\n",
    "___\n",
    "\n",
    "Ayant compris comment les r√©seaux de neurones utilisent des poids et des biais pour effectuer des calculs, il est temps d'explorer comment ils introduisent de la non-lin√©arit√© et de la complexit√© dans ces calculs. C'est l√† que les **fonctions d'activation** entrent en jeu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examinons la formule du neurone :\n",
    "\n",
    "√âtant donn√© un neurone ùëñ, sa sortie $y_i$ est calcul√©e par :\n",
    "\n",
    "### $y_i = \\phi\\left(\\sum_{j=1}^{n} w_{ij} x_j + b_i\\right)$\n",
    "\n",
    "O√π :\n",
    "\n",
    "- $x_j$ sont les valeurs d'entr√©e du neurone.\n",
    "- $w_{ij}$ sont les poids associ√©s aux entr√©es.\n",
    "- $b_i$ est le terme de biais pour le neurone.\n",
    "- $\\sum_{j=1}^{n} w_{ij} x_j$ est la somme pond√©r√©e des entr√©es.\n",
    "- $\\phi(\\cdot)$ est **la fonction d'activation** appliqu√©e √† la somme pond√©r√©e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mais qu'est-ce qu'une fonction d'activation ?\n",
    "\n",
    "Dans les r√©seaux de neurones, une fonction d'activation est une fonction math√©matique appliqu√©e √† la sortie d'un neurone, introduisant de la non-lin√©arit√© dans le mod√®le (pensez √† la lin√©arit√© comme $y = ax + b$, c'est-√†-dire une droite). Cette non-lin√©arit√© permet au r√©seau de capturer et de repr√©senter des motifs complexes dans les donn√©es, ce qui serait impossible si le r√©seau ne r√©alisait que des transformations lin√©aires. Sans fonctions d'activation, m√™me avec plusieurs couches, le r√©seau agirait essentiellement comme un mod√®le de r√©gression lin√©aire, limit√© √† mod√©liser des relations lin√©aires.\n",
    "\n",
    "La fonction d'activation d√©termine si un neurone doit √™tre \"activ√©\". En appliquant une transformation non lin√©aire, la fonction d'activation permet au neurone de contribuer √† l'apprentissage de correspondances complexes et non lin√©aires entre les entr√©es et les sorties.\n",
    "\n",
    "Si un r√©seau de neurones n'utilisait que des transformations lin√©aires (√† travers des multiplications et des additions de matrices), l'empilement de plusieurs couches donnerait toujours un mod√®le qui produit une fonction lin√©aire de ses entr√©es. La fonction d'activation non lin√©aire est donc essentielle pour la capacit√© du r√©seau √† approximer des motifs complexes du monde r√©el.\n",
    "\n",
    "![alt text](../../Source/linearregressionvsnonlinear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'ennemi jur√© d'une fonction d'activation\n",
    "\n",
    "Le pire ennemi d'une fonction d'activation est le **probl√®me du gradient √©vanescent**. *(√âvanescent : Qui s'amoindrit et dispara√Æt graduellement)*\n",
    "\n",
    "Cela se produit lorsque les gradients, qui sont utilis√©s pour mettre √† jour les poids lors de la r√©tropropagation, deviennent extr√™mement petits √† mesure qu'ils se propagent √† travers les couches d'un r√©seau de neurones. En cons√©quence, le r√©seau apprend tr√®s lentement ou cesse m√™me d'apprendre compl√®tement.\n",
    "\n",
    "Le probl√®me du gradient explosif est un autre d√©fi majeur dans l'entra√Ænement des r√©seaux de neurones profonds.\n",
    "\n",
    "Il se produit lorsque les gradients deviennent excessivement grands lors de la r√©tropropagation, entra√Ænant un entra√Ænement instable et pouvant potentiellement faire diverger les poids du r√©seau.\n",
    "\n",
    "*Nous aborderons les gradients plus tard, pour l'instant, rappelez-vous simplement qu'ils indiquent les corrections √† apporter lors de l'entra√Ænement d'un mod√®le :)*\n",
    "\n",
    "![alt text](../../Source/vanishing-and-exploding-gradient-1.webp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from numpy import exp\n",
    "\n",
    "# d√©finir les donn√©es d'entr√©e\n",
    "inputs = [x for x in range(-10, 10)]\n",
    "\n",
    "# ex√©cutez simplement ce code pour l'initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choisir une fonction d'activation\n",
    "\n",
    "Voici une liste non exhaustive de certaines fonctions d'activation tr√®s courantes, leur cas d'usage et leurs formules respectives :\n",
    "\n",
    "***Votre objectif est d'essayer de recr√©er chaque fonction math√©matiquement et de la tracer pour obtenir le m√™me graphique que l'exemple fourni !***\n",
    "- **Sigmo√Øde :**\n",
    "\n",
    "    La fonction sigmo√Øde peut traiter n'importe quel nombre r√©el et le mapper entre 0 et 1. Ce mappage la rend utile dans les probl√®mes de classification binaire en apprentissage automatique, o√π la sortie est mod√©lis√©e comme une probabilit√©.\n",
    "$$\\sigma(x) = \\frac{1} {1 + e^{-x}}$$ \n",
    "![alt text](../../Source/sigmoid_function.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # TODO: Impl√©mentez la fonction sigmoid\n",
    "    return ...\n",
    "\n",
    "sigmoid_outputs = [sigmoid(x) for x in inputs]\n",
    "\n",
    "print(\"SIGMOID FUNCTION\")\n",
    "pyplot.plot(inputs, sigmoid_outputs)\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- **Tanh (Tangente Hyperbolique) :**\n",
    "\n",
    "    Historiquement, la fonction tanh est devenue pr√©f√©r√©e √† la fonction sigmo√Øde car elle offrait de meilleures performances pour les r√©seaux de neurones √† couches multiples. Mais elle n'a pas r√©solu le probl√®me du gradient √©vanescent dont souffraient les sigmo√Ødes, probl√®me qui a √©t√© abord√© de mani√®re plus efficace avec l'introduction des activations ReLU.\n",
    "$$tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} = \\frac{1 - e^{-2x}}{1 + e^{-2x}}$$\n",
    "\n",
    "![alt text](../../Source/tanh_function.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    # TODO: Impl√©mentez la fonction tanh\n",
    "    return ...\n",
    "\n",
    "tanh_outputs = [tanh(x) for x in inputs]\n",
    "print(\"TANH FUNCTION\")\n",
    "pyplot.plot(inputs, tanh_outputs)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- **ReLU (Unit√© Lin√©aire Rectifi√©e) :**\n",
    "\n",
    "    Au cours des derni√®res ann√©es, l'unit√© lin√©aire rectifi√©e (ReLU) a d√©pass√© la fonction sigmo√Øde en popularit√© en tant que fonction d'activation dans les r√©seaux de neurones. ReLU am√©liore les r√©sultats concernant le probl√®me du gradient √©vanescent, s'adapte bien aux grands r√©seaux et offre un calcul plus rapide gr√¢ce √† sa formule plus simple.\n",
    "$$ReLU(x) = \\max(0, x)$$\n",
    "![alt text](../../Source/relu_function.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    # TODO: Impl√©mentez la fonction relu\n",
    "    return ...\n",
    "\n",
    "relu_outputs = [relu(x) for x in inputs]\n",
    "print(\"RELU FUNCTION\")\n",
    "pyplot.plot(inputs, relu_outputs)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- **Leaky ReLU :**\n",
    "\n",
    "    Bien que ReLU repr√©sente une am√©lioration significative par rapport aux fonctions d'activation traditionnelles comme la sigmo√Øde et la tanh, elle pr√©sente encore des limitations lorsqu'il s'agit de tr√®s profonds r√©seaux de neurones. Si l'entr√©e d'un neurone ReLU est n√©gative, sa sortie est z√©ro. Si cela se produit de mani√®re r√©p√©t√©e, le neurone peut devenir \"mort\" et ne jamais se r√©activer. Cela peut emp√™cher le neurone d'apprendre et de contribuer √† la performance du r√©seau.\n",
    "    Pour r√©soudre ces probl√®mes, les chercheurs ont d√©velopp√© diverses techniques, comme Leaky ReLU, qui introduit une petite pente pour les entr√©es n√©gatives, emp√™chant ainsi les neurones de devenir compl√®tement morts.\n",
    "$$LeakyReLU(x) = \\max(\\alpha * x, x)$$\n",
    "\n",
    "![alt text](../../Source/leaky_relu_function.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x): # on utilisera alpha = 0.1\n",
    "    # TODO: Impl√©mentez la fonction leaky_relu\n",
    "    return ...\n",
    "\n",
    "leaky_relu_outputs = [leaky_relu(x) for x in inputs]\n",
    "print(\"LEAKY_RELU FUNCTION\")\n",
    "pyplot.plot(inputs, leaky_relu_outputs)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- **Softmax :**\n",
    "\n",
    "    La fonction Softmax est une fonction d'activation couramment utilis√©e dans la couche de sortie d'un r√©seau de neurones pour les probl√®mes de classification multi-classe. Elle prend un vecteur de nombres r√©els en entr√©e et le normalise en une distribution de probabilit√©, o√π la somme des probabilit√©s est √©gale √† 1.\n",
    "    Par exemple, le softmax standard de (1,2,8) est approximativement (0.001, 0.002, 0.997), ce qui revient √† attribuer presque tout le poids unitaire total dans le r√©sultat √† la position de l'√©l√©ment maximal du vecteur (de 8).\n",
    "    La fonction Softmax est d√©finie comme suit :\n",
    "$$\\sigma(x_i) = \\frac{e^{x_{i}}}{\\sum_{j=1}^K e^{x_{j}}} \\ \\ \\ \\text{pour } i=1,2,\\dots,K$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_inputs = [1.0, 3.0, 2.0]\n",
    "\n",
    "def softmax(x): # nous allons print celui-ci plutot que de le dessiner\n",
    "    # TODO: Impl√©mentez la fonction softmax\n",
    "    return ...\n",
    "\n",
    "softmax_outputs = softmax(softmax_inputs)\n",
    "print(\"SOFTMAX FUNCTION\")\n",
    "print(softmax_outputs)\n",
    "assert softmax_outputs.sum() == 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dans quel but utilisons-nous les fonctions d'activation ?\n",
    "\n",
    "Le choix de la fonction d'activation a un impact consid√©rable sur la capacit√© et la performance du r√©seau de neurones, et diff√©rentes fonctions d'activation peuvent √™tre utilis√©es dans diff√©rentes parties du mod√®le.\n",
    "\n",
    "Un r√©seau peut avoir trois types de couches : **couches d'entr√©e** qui prennent des donn√©es brutes du domaine, **couches cach√©es** qui prennent des entr√©es d'une autre couche et passent des sorties √† une autre couche, et **couches de sortie** qui font une pr√©diction.\n",
    "\n",
    "Toutes les couches cach√©es utilisent g√©n√©ralement la m√™me fonction d'activation. La couche de sortie utilisera g√©n√©ralement une fonction d'activation diff√©rente de celle des couches cach√©es et d√©pendra du type de pr√©diction requise par le mod√®le.\n",
    "\n",
    "Il y a peut-√™tre trois fonctions d'activation que vous voudrez consid√©rer pour une utilisation dans la couche de sortie ; elles sont :\n",
    "\n",
    "- **Lin√©aire :** typiquement $f(x) = x$\n",
    "- **Sigmo√Øde :** utile pour la classification binaire\n",
    "- **Softmax :** utile pour attribuer des scores de probabilit√©, c'est celle utilis√©e par les transformateurs (ex: ChatGPT)\n",
    "\n",
    "Ce n'est pas une liste exhaustive des fonctions d'activation utilis√©es pour les couches de sortie, mais ce sont les plus couramment utilis√©es.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# ***Bonus***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essayons de visualiser l'impact qu'une fonction d'activation peut avoir sur l'entra√Ænement d'un mod√®le :\n",
    "\n",
    "L'objectif de cette activit√© est d'essayer de reproduire le probl√®me du gradient √©vanescent.\n",
    "\n",
    "Le code suivant g√©n√®re deux groupes distincts de points de donn√©es sous la forme de croissants de lune. Il construit ensuite un r√©seau de neurones qui sera entra√Æn√© √† diff√©rencier si un point de donn√©es appartient √† un croissant de lune ou √† l'autre.\n",
    "\n",
    "Votre t√¢che est de lancer le code et de voir si la perte diminue au cours des √©poques. Vous pourriez observer peu ou pas de changements dans la valeur de la perte, auquel cas vous √™tes probablement confront√© √† un probl√®me de gradient √©vanescent. Pouvez-vous trouver comment le r√©soudre ? (Gardez √† l'esprit qu'il peut √™tre n√©cessaire d'essayer plusieurs fois en raison de la randomisation.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons # jeux de donn√©es de classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "X, y = make_moons(n_samples=250, noise=0.05, random_state=42) # cr√©er nos 2 lunes\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=100)\n",
    "plt.show()\n",
    "\n",
    "# construction d'un r√©seau de neurones complexe avec deux entr√©es et neuf couches avec 10 n≈ìuds\n",
    "model = Sequential()\n",
    "\n",
    "# prend les coordonn√©es x et y d'un point de donn√©es comme entr√©e\n",
    "model.add(Dense(10, activation='sigmoid', input_dim=2))\n",
    "\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "# la sortie utilise √©galement une fonction d'activation sigmo√Øde car elle convient √† la classification binaire (0-0.5 et 0.5-1)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.get_weights()[0]\n",
    "\n",
    "old_weights = model.get_weights()[0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=300)\n",
    "\n",
    "new_weights = model.get_weights()[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
