{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Le neurone artificiel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "![alt text](../../Source/neurone_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce diagramme illustre la structure de base d'un neurone artificiel unique, souvent appelé perceptron, qui est un élément fondamental des réseaux de neurones.\n",
    "\n",
    "### Explication du Diagramme :\n",
    "1. **Entrées $(X_1, X_2, \\dots, X_n)$** :\n",
    "   - Ce sont les caractéristiques ou signaux donnés en entrée dans le neurone. Chaque entrée a un **poids** (ou coefficient) associé ($W_1, W_2, \\dots, W_n$) qui détermine l'importance ou la contribution de cette entrée à la sortie du neurone.\n",
    "\n",
    "2. **Poids $(W_1, W_2, \\dots, W_n)$** :\n",
    "   - Ce sont des valeurs scalaires qui se multiplient avec les entrées correspondantes. Les poids sont cruciaux car ils ajustent les valeurs d'entrée, influençant la manière dont elles contribuent à la décision ou à la prédiction finale.\n",
    "\n",
    "3. **Biais $(b)$** :\n",
    "   - Le biais est un paramètre supplémentaire qui permet au modèle de mieux s'adapter aux données. Il aide à décaler la fonction d'activation vers la gauche ou la droite, ce qui peut être crucial pour le processus d'apprentissage.\n",
    "\n",
    "4. **Fonction de Somme $(Σ)$** :\n",
    "   - C'est ici que toutes les entrées pondérées sont additionnées avec le biais. Mathématiquement, cela peut être représenté comme suit :\n",
    "     $a = (W_1 \\cdot X_1) + (W_2 \\cdot X_2) + \\dots + (W_n \\cdot X_n) + b$\n",
    "   - Ici, le résultat $a$ représente l'entrée qui sera ensuite passé dans la fonction d'activation.\n",
    "\n",
    "5. **Fonction d'Activation $F(a)$** :\n",
    "   - La sortie de la fonction de somme est ensuite passée à travers une fonction d'activation $F(a)$. Cette fonction introduit une non-linéarité dans le modèle, permettant au réseau d'apprendre et de modéliser des motifs complexes dans les données. Les fonctions d'activation courantes incluent les fonctions Sigmoïde, ReLU et Tanh.\n",
    "\n",
    "6. **Sortie $(Y)$** :\n",
    "   - Après que la fonction d'activation ait été appliquée, la sortie finale $Y$ est produite. Cette sortie peut représenter une décision, comme la classification d'une entrée, ou dans des modèles plus complexes, elle peut être utilisée comme entrée pour un autre neurone dans un réseau à plusieurs couches.\n",
    "\n",
    "### Résumé :\n",
    "Ce diagramme représente le fonctionnement d'un réseau de neurones simple, montrant comment les entrées sont pondérées, additionnées, puis traitées à travers une fonction d'activation pour produire une sortie. Ce processus est répété à travers de nombreux neurones et couches pour permettre au réseau de neurones d'apprendre et de faire des prédictions.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
