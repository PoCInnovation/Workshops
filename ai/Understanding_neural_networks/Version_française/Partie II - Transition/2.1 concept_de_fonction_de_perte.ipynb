{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 ***Ce qui suit : Le Rôle des Fonctions de Perte***\n",
    "___\n",
    "\n",
    "Maintenant que nous avons vu comment les réseaux de neurones utilisent des couches et des poids pour faire des prédictions, la question suivante est : **comment le réseau sait-il si ses prédictions sont correctes ?** C'est ici qu'entre en jeu le concept de **fonction de perte**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qu'est-ce qu'une Fonction de Perte ?\n",
    "\n",
    "Une fonction de perte est un moyen de mesurer dans quelle mesure les prédictions du réseau de neurones correspondent aux valeurs cibles réelles. Elle fournit une valeur numérique indiquant le degré d'erreur dans les prédictions. Plus la perte est faible, plus les prédictions du réseau sont proches des cibles réelles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../../Source/loss_understand.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe de nombreuses fonctions de perte adaptées à différents cas, mais aujourd'hui nous allons nous concentrer sur l'erreur quadratique moyenne, que nous appellerons ici **Mean Squared Error (MSE)**, qui est l'une des fonctions de perte les plus simples à comprendre. Voici sa représentation mathématique :\n",
    "\n",
    "<img src=\"../../Source/loss_graph.png\" alt=\"alt text\" width=\"450\" height=\"300\">\n",
    "\n",
    "\n",
    "### Représentation Mathématique de la MSE\n",
    "\n",
    "La Mean Squared Error (MSE) mesure la différence moyenne au carré entre les valeurs prédites et les valeurs cibles réelles. Elle est calculée à l'aide de la formule suivante :\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Où :\n",
    "\n",
    "- $n$ est le nombre de points de données.\n",
    "- $y_i$ est la valeur cible réelle pour le $i^{\\text{ème}}$ point de données.\n",
    "- $ŷ_i$ est la valeur prédite pour le $i^{\\text{ème}}$ point de données.\n",
    "\n",
    "*Elle représente la différence moyenne au carré entre les valeurs prédites et les valeurs cibles réelles.*\n",
    "***Voici une [vidéo](https://www.youtube.com/watch?v=VaOlkbKQFcY) pour mieux comprendre***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Essayons d'implémenter la fonction MSE !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "target_output = np.array([1.0, 0.0, 1.0, 0.0])\n",
    "\n",
    "predicted_output = np.array([0.9, 0.5, 0.8, 0.2])\n",
    "\n",
    "# TODO : Calculer la Mean Squared Error\n",
    "mse_loss = np.mean(...)\n",
    "\n",
    "print(\"Mean Squared Error\", mse_loss)\n",
    "assert mse_loss == 0.08499999999999999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien joué ! Vous comprenez maintenant comment fonctionne une fonction de perte ! PyTorch implémente de nombreuses fonctions de perte différentes, comme la **Mean Squared Error** et la **Cross-Entropy Loss**, qui est l'une des fonctions de perte les plus couramment utilisées pour les tâches de classification.\n",
    "\n",
    "Comme vous pouvez le voir ci-dessous, voici comment vous pouvez utiliser ces fonctions de perte dans PyTorch :\n",
    "\n",
    "**Essayez de l'implémenter !**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Exemple utilisant la fonction de perte Mean Squared Error (MSE) de torch\n",
    "mse_loss = ...\n",
    "\n",
    "# Exemple utilisant la fonction de perte Cross-Entropy de torch\n",
    "cross_entropy_loss = ...\n",
    "\n",
    "target_output = torch.tensor([1.0, 0.0, 1.0, 0.0])\n",
    "predicted_output = torch.tensor([0.9, 0.5, 0.8, 0.2])\n",
    "\n",
    "result_mse = mse_loss(predicted_output, target_output)\n",
    "print(\"MSE Loss :\", result_mse)\n",
    "\n",
    "result_cross_entropy = cross_entropy_loss(predicted_output, target_output)\n",
    "print(\"Cross-Entropy Loss :\", result_cross_entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# ***Bonus***\n",
    "### Comprendre la Fonction de Perte Cross-Entropy\n",
    "\n",
    "La **Cross-Entropy Loss** est une fonction de perte cruciale, utilisée principalement pour les tâches de classification. Elle mesure la différence entre la distribution réelle des classes et la distribution prédite, ce qui en fait un excellent choix pour les tâches où l'objectif est de prédire la catégorie ou la classe à laquelle appartient une entrée.\n",
    "\n",
    "### Comment Fonctionne la Cross-Entropy Loss\n",
    "\n",
    "- **Représentation de la Cible** : Pour la classification, la sortie cible est souvent représentée sous forme de vecteur encodé en one-hot. Par exemple, s'il y a trois classes et que la bonne classe est la deuxième, le vecteur cible ressemblera à \\([0, 1, 0]\\).\n",
    "\n",
    "- **Probabilités Prédites** : Le modèle génère un vecteur de probabilités prédites pour chaque classe, qui doit totaliser 1. Par exemple, la prédiction pourrait être \\([0.2, 0.7, 0.1]\\).\n",
    "\n",
    "- **Calcul** : La Cross-Entropy Loss mesure la différence entre la classe correcte (le vecteur one-hot) et les probabilités prédites. La formule est :\n",
    "\n",
    "  $$\n",
    "  L = -\\sum_{i=1}^{n} y_i \\cdot \\log(\\hat{y}_i)\n",
    "  $$\n",
    "\n",
    "  Où :\n",
    "  - $y_i$ est la valeur cible réelle (1 pour la classe correcte, 0 sinon).\n",
    "  - $ŷ_i$ est la probabilité prédite pour chaque classe.\n",
    "\n",
    "### Pourquoi Utiliser la Cross-Entropy Loss ?\n",
    "\n",
    "- **Meilleure pour la Classification** : Contrairement à la Mean Squared Error, qui mesure les différences au carré, la Cross-Entropy Loss compare directement les distributions de probabilités, ce qui la rend plus adaptée aux problèmes de classification. Elle pénalise fortement les prédictions incorrectes, incitant le modèle à produire des prédictions précises et confiantes.\n",
    "\n",
    "### Résumé\n",
    "\n",
    "La Cross-Entropy Loss est un outil puissant pour l'entraînement des modèles de classification, car elle mesure directement la correspondance entre les probabilités prédites et les classes réelles. En minimisant cette perte, le modèle apprend à faire des prédictions plus précises.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
