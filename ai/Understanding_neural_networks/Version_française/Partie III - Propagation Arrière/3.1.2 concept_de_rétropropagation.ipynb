{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.2 ***Rétropropagation***\n",
    "___\n",
    "\n",
    "Bien que le concept de gradient nous indique la direction dans laquelle nous devons ajuster chaque paramètre (l'augmenter ou le diminuer), jusqu'à présent, nous savons uniquement comment calculer le gradient pour la couche de sortie.\n",
    "\n",
    "La **rétropropagation** est l'algorithme chargé de transmettre le gradient de la couche de sortie jusqu'à la couche d'entrée, et il le fait en utilisant une technique de calcul appelée **règle de la chaîne**, ou \"théorème de dérivation des fonctions composées\" (que vous avez probablement déjà vu au lycée).\n",
    "\n",
    "##### Approche intuitive :\n",
    "\n",
    "_\"Si une voiture va deux fois plus vite qu'un vélo et que le vélo va quatre fois plus vite qu'un piéton, alors la voiture se déplace 2 × 4 = 8 fois plus vite que le piéton.\"_\n",
    "\n",
    "Le lien entre cet exemple et la règle de la chaîne est le suivant. Soient $z$, $y$, et $x$ les positions (variables) de la voiture, du vélo et du piéton, respectivement. Le taux de variation des positions relatives de la voiture et du vélo est $\\frac{dz}{dy} = 2.$\n",
    "\n",
    "De même, $\\frac{dy}{dx} = 4.$\n",
    "\n",
    "Ainsi, le taux de variation des positions relatives de la voiture et du piéton est\n",
    "\n",
    "$\n",
    "\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx} = 2 \\cdot 4 = 8.\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puisque chaque couche du réseau de neurones prend en entrée la sortie de la couche précédente, notre réseau de neurones ressemble fondamentalement à $f(g(h(i(j(x)))))$ avec $x$ comme entrée et $f$ comme couche de sortie. En calculant le gradient de $f$, nous nous posons la question _\"comment un changement dans $g(h(i(j(x))))$ affecte-t-il la sortie de $f$\"_. Nous voulons pouvoir reproduire cette même étape avec $g$ et $h(i(j(x)))$, etc.\n",
    "\n",
    "Pour faire court, en multipliant les dérivées d'une fonction composée à chaque étape, on obtient le taux de variation global de la fonction par rapport à la variable initiale.\n",
    "\n",
    "En d'autres termes, la règle de la chaîne nous permet de calculer comment une modification de la variable d'entrée se répercute à travers chaque étape d'une fonction composée, simplement en multipliant les dérivées de chaque étape.\n",
    "\n",
    "Nous pouvons utiliser la relation vue ci-dessus pour affirmer que, pour une fonction composée $f(g(h(i(j(x)))))$, le gradient $\\frac{\\partial f(g(h(i(j(x))))) }{\\partial x}$ peut être calculé comme suit :\n",
    "\n",
    "$\n",
    "\\frac{\\partial f(g(h(i(j(x))))) }{\\partial x} = \\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial g} \\times \\frac{\\partial g}{\\partial h} \\times \\frac{\\partial h}{\\partial i} \\times \\frac{\\partial i}{\\partial j} \\times \\frac{\\partial j}{\\partial x}\n",
    "$\n",
    "\n",
    "C'est sur cette base que repose la rétropropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si c'est la première fois que vous apprenez la rétropropagation, cela peut sembler très complexe. Ne vous inquiétez pas, nous ne vous demanderons pas de l'implémenter, mais c'est un bon objectif que d'essayer de visualiser ce qu'elle fait vraiment.\n",
    "\n",
    "[Retour à la Fonction d'Optimisation !](<3.1 concept_de_fonction_d'optimisation.ipynb>)\n",
    "\n",
    "Si vous voulez aller plus loin, voici une liste de vidéos/sites utiles pour comprendre ce sujet plus en profondeur :\n",
    "- https://www.youtube.com/watch?v=SmZmBKc7Lrs&t=282s\n",
    "- https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&si=eCb-husS-sTwthq-\n",
    "- https://ai.plainenglish.io/calculus-for-backpropagation-doesnt-have-to-be-scary-16595d76e744\n",
    "- https://en.wikipedia.org/wiki/Chain_rule\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
