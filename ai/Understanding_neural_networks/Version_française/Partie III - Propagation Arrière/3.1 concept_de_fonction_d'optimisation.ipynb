{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 ***La Fonction d'Optimisation***\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fonction d'optimisation dans un réseau de neurones est un algorithme qui ajuste les poids du modèle pour minimiser l'erreur (ou perte) entre les prédictions du réseau et les valeurs cibles réelles. Les fonctions d'optimisation courantes incluent la **Descente de Gradient** et ses variantes, comme **Adam** et **RMSprop**.\n",
    "\n",
    "Aujourd'hui, nous allons nous concentrer sur celle qui est peut-être la plus simple : la **Descente de Gradient**\n",
    "\n",
    "Avant de passer à l'activité ci-dessous, familiarisez-vous avec certains concepts clés en réalisant les deux activités suivantes :\n",
    "- [Qu'est-ce qu'un gradient et comment descend-il ?](<3.1.1 la_descente_de_gradient.ipynb>)\n",
    "- [Rétropropagation ou comment calculer les gradients ?](<3.1.2 concept_de_rétropropagation.ipynb>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En résumé :\n",
    "- Un **gradient** nous indique dans quelle direction ajuster nos paramètres.\n",
    "- La **rétropropagation** nous permet de calculer les gradients depuis la couche de sortie jusqu'à l'entrée de notre modèle.\n",
    "- La **Descente de Gradient** est un algorithme qui ajustera de manière itérative les paramètres de notre modèle afin de minimiser l'erreur (perte) de nos prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "target = np.array([2.0, 3.0, 4.0, 5.0])\n",
    "\n",
    "W1 = np.array([[-0.2416, -0.2497, -0.3932,  0.2935],\n",
    "                [-0.0396,  0.1421, -0.4436,  0.1714],\n",
    "                [-0.3519,  0.4072, -0.0721, -0.1659],\n",
    "                [ 0.2055, -0.0330, -0.2145,  0.1987]]) \n",
    "# poids pour la couche d'entrée qui sont initialisés de manière aléatoire pour un exemple, modifiez-les si vous souhaitez tester avec plus d'exemples\n",
    "\n",
    "# TODO : faire la prédiction (rappelez-vous de la section 1.1)\n",
    "prediction = np.dot(W1, input)  # en supposant que l'opération prévue est un produit scalaire (dot product)\n",
    "\n",
    "print(\"prediction\", prediction)\n",
    "assert prediction.sum() == -0.8516999999999999\n",
    "\n",
    "######################################################################\n",
    "loss = np.mean((prediction - target) ** 2) # Mean Squared Error (MSE)\n",
    "print(\"loss\", loss)\n",
    "\n",
    "gradient = np.mean((prediction - target) * input)\n",
    "print(\"gradient\", gradient)\n",
    "# Le gradient ici est calculé en fonction de l'entrée et de la sortie,\n",
    "# nous fournissant la pente de la fonction de perte par rapport aux valeurs d'entrée !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons un gradient, essayons de minimiser la perte **en dessous de 2.96**. Faites de votre mieux !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : mettre à jour les poids avec le gradient\n",
    "LEARNING_RATE = ...  # définir un taux d'apprentissage\n",
    "New_W1 = ...\n",
    "\n",
    "prediction = New_W1 @ input\n",
    "loss = np.mean((prediction - target) ** 2)\n",
    "print(\"new loss\", loss)  # devrait être inférieur à la perte précédente\n",
    "\n",
    "W1 = New_W1\n",
    "# N'hésitez pas à exécuter le code plusieurs fois pour voir la perte diminuer ou augmenter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"final W1\", W1)\n",
    "print(\"-\" * 55)\n",
    "prediction = input @ W1\n",
    "\n",
    "print(\"prediction\", prediction)\n",
    "# Notez que les prédictions sont beaucoup plus proches des cibles que la première prédiction !\n",
    "# Mais pas encore très proches... Si vous voulez vous rapprocher davantage, essayez avec une autre fonction de perte :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent travail ! Rappelez-vous, lors de la mise à jour des poids en utilisant le gradient, vous appliquez généralement une fraction du gradient, et cette fraction est contrôlée par le **taux d'apprentissage**. Vous pouvez définir le taux d'apprentissage manuellement ou utiliser des algorithmes avancés qui l'ajustent automatiquement.\n",
    "\n",
    "Vous comprenez maintenant comment un modèle apprend et s'adapte en mettant à jour ses poids via la descente de gradient. Gardez à l'esprit que le gradient doit être recalculé et réinitialisé au début de chaque époque d'entraînement pour continuer à affiner le modèle de manière efficace.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
