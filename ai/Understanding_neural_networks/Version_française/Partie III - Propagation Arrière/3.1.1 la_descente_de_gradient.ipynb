{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.1 ***Descente de Gradient***\n",
    "\n",
    "Nous nous sommes déjà familiarisés avec le concept de **fonction de perte**. Elle nous aide à quantifier l'écart entre nos prédictions et les valeurs réelles. L'étape suivante consiste à savoir quels ajustements apporter à nos poids et biais afin de minimiser la fonction de perte. C'est là que la **descente de gradient** intervient.\n",
    "La **descente de gradient** fonctionne en ajustant de manière itérative les paramètres du modèle (poids et biais) dans la direction qui diminue la fonction de perte.\n",
    "\n",
    "Simplifions d'abord le problème à une seule dimension pour le rendre plus accessible :\n",
    "\n",
    "Minimiser la fonction de perte signifie trouver un minimum local. *Quelle valeur de $x$ me donne le $y$ le plus bas ?*\n",
    "\n",
    "Ceux d'entre vous qui sont familiers avec le calcul différentiel savent que nous pouvons déterminer la pente de la fonction à n'importe quel point en trouvant la dérivée. La dérivée nous indique la pente de la fonction et dans quelle direction elle s'incline : si elle est croissante ou décroissante. Si la pente (ou dérivée) est positive, cela signifie que la fonction est croissante à ce point, donc pour minimiser la perte, nous devrions aller dans la direction négative. À l'inverse, si la pente est négative, la fonction est décroissante, et nous devrions avancer dans la direction positive pour atteindre le minimum. Pensez-y comme à une balle qui roule en bas d'une colline.\n",
    "\n",
    "![alt text](../../Source/youtube-video-gif.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le contexte d'un réseau de neurones, nous traitons des fonctions de perte avec bien plus d'un seul paramètre.\n",
    "\n",
    "Le terme \"**gradient**\" fait référance à une généralisation de la dérivée en plusieurs dimensions. Lorsqu'on traite des fonctions à plusieurs variables, comme une fonction $f(x_1, x_2, \\dots, x_n)$, le **gradient** est un vecteur qui contient toutes les dérivées partielles de la fonction par rapport à chacune de ses variables.\n",
    "\n",
    "Par exemple, si nous avons une fonction $f(x, y)$, le gradient de $f$, noté $\\nabla f$, est un vecteur qui comprend deux composantes :\n",
    "\n",
    "$\\nabla f = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right)$\n",
    "\n",
    "Chaque composante nous indique le taux de changement de la fonction par rapport à l'une des variables, en gardant les autres constantes.\n",
    "\n",
    "À chaque itération, nous mettrons à jour nos paramètres en les déplaçant dans la direction opposée du **gradient**, pondéré par un facteur appelé le **taux d'apprentissage**. La formule générale pour la descente de gradient est :\n",
    "\n",
    "$\\theta_{t+1} = \\theta_t - \\alpha \\nabla_{\\theta} L(\\theta_t)$\n",
    "\n",
    "Où :\n",
    "\n",
    "- $\\theta_t$ représente le vecteur de paramètres à l'itération $t$.\n",
    "- $\\alpha$ est le **taux d'apprentissage**, un scalaire positif arbitraire qui détermine la taille des pas que nous faisons dans la direction du gradient négatif.\n",
    "- $\\nabla_{\\theta} L(\\theta_t)$ est le gradient de la fonction de perte $L(\\theta)$ par rapport aux paramètres $\\theta$ à l'itération $t$. Ce gradient est un vecteur où chaque composante est la dérivée partielle de la fonction de perte par rapport à un paramètre spécifique de $\\theta$.\n",
    "\n",
    "![alt text](<../../Source/gradient descent.png>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Essayons de mettre en oeuvre notre descente de gradient itérative avec une fonction 1D simple\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Définir la fonction à minimiser\n",
    "def function(x):\n",
    "    # TODO : Implémenter la fonction f(x) = x^2 + 4*x + 4\n",
    "    return ...\n",
    "\n",
    "\n",
    "# Définir le gradient de la fonction comme sa dérivée\n",
    "def gradient(x):\n",
    "    # TODO : Implémenter le gradient de la fonction ci-dessus\n",
    "    return ...\n",
    "\n",
    "assert function(0) == 4, \"Erreur dans l'implémentation de la fonction pour x=0\"\n",
    "assert function(1) == 9, \"Erreur dans l'implémentation de la fonction pour x=1\"\n",
    "assert function(-2) == 0, \"Erreur dans l'implémentation de la fonction pour x=-2\"\n",
    "\n",
    "assert gradient(0) == 4, \"Erreur dans l'implémentation du gradient pour x=0\"\n",
    "assert gradient(1) == 6, \"Erreur dans l'implémentation du gradient pour x=1\"\n",
    "assert gradient(-2) == 0, \"Erreur dans l'implémentation du gradient pour x=-2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_current = 10  # Point de départ\n",
    "learning_rate = 0.1\n",
    "num_iterations = 20\n",
    "\n",
    "# Listes pour stocker les valeurs pour la visualisation\n",
    "x_values = [x_current]\n",
    "f_values = [function(x_current)]\n",
    "\n",
    "# Boucle de descente de gradient\n",
    "for _ in range(num_iterations):\n",
    "    x_current -= learning_rate * gradient(x_current)\n",
    "    x_values.append(x_current)\n",
    "    f_values.append(function(x_current))\n",
    "\n",
    "# Visualisation\n",
    "x_range = np.linspace(-10, 10, 400)\n",
    "plt.plot(x_range, function(x_range), 'r-', label='Fonction f(x)')\n",
    "plt.plot(x_values, f_values, 'bo-', label='Chemin de la Descente de Gradient')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Visualisation de la Descente de Gradient')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Retour à la Fonction d'Optimisation !](<3.1 concept_de_fonction_d'optimisation.ipynb>)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
