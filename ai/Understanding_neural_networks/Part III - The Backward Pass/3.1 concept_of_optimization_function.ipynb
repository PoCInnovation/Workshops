{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 ***The Optimization Function***\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An optimization function in a neural network is an algorithm that adjusts the modelâ€™s weights to minimize the error (or loss) between the network's predictions and the actual target values. Common optimization functions include **Gradient Descent** and its variants, like **Adam** and **RMSprop**.\n",
    "\n",
    "Today we will focus on perhaps the simplest one: **Gradient Descent**\n",
    "\n",
    "\n",
    "Before doing the activity down below, familiarize yourself with some key concepts by doing the following 2 activities:\n",
    "- [What is a gradient and how does it descend?](<3.1.1 gradient_descent.ipynb>)\n",
    "- [Backpropagation or how to calculate the gradients?](<3.1.2 concept_of_backpropagation.ipynb>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary:\n",
    "- A **gradient** tells us in what direction to adjust our parameters.\n",
    "- **Backpropagation** allows us to compute the gradients from the output layer all the way back to the input of our model.\n",
    "- **Gradient Descent** is an algorithm which will iteratively adjust the parameters of our model in order to minimize the error (loss) of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "input = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "target = np.array([2.0, 3.0, 4.0, 5.0])\n",
    "\n",
    "W1 = np.array([[-0.2416, -0.2497, -0.3932,  0.2935],\n",
    "                [-0.0396,  0.1421, -0.4436,  0.1714],\n",
    "                [-0.3519,  0.4072, -0.0721, -0.1659],\n",
    "                [ 0.2055, -0.0330, -0.2145,  0.1987]]) \n",
    "# weights for the input layer that are randomly initialized for 1 example, change if you want to test with more examples\n",
    "\n",
    "#TODO: make the predicition (remember the 1.1)\n",
    "prediction = ...\n",
    "\n",
    "print(\"prediction\", prediction)\n",
    "assert prediction.sum() == -0.8516999999999999\n",
    "\n",
    "######################################################################\n",
    "loss = np.mean((prediction - target) ** 2) # mean squared error\n",
    "print(\"loss\", loss)\n",
    "\n",
    "gradient = np.mean((prediction - target) * input)\n",
    "print(\"gradient\", gradient)\n",
    "# The gradient here is calculated based on the input and output,\n",
    "# providing the slope of the loss function with respect to the input values !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a gradient let's try to minimize the loss **below 2.96**, try your best !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : update the weights with the gradient\n",
    "LEARNING_RATE = ...\n",
    "New_W1 = ...\n",
    "\n",
    "prediction = New_W1 @ input\n",
    "loss = np.mean((prediction - target) ** 2)\n",
    "print(\"new loss\", loss) # should be smaller than the previous loss\n",
    "\n",
    "W1 = New_W1\n",
    "# Don't hesitate to run the code multiple times to see the loss decreasing or increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"final W1\", W1)\n",
    "print(\"-\" * 55)\n",
    "prediction = input @ W1\n",
    "\n",
    "print(\"prediction\", prediction)\n",
    "# Note that the prediction are much closer to the target than the first prediction !\n",
    "# But not yet very close... If you want to be closer try with another loss function :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! Remember, when updating the weights using the gradient, you typically apply a fraction of the gradient, this fraction is controlled by the **learning rate**. You can set the learning rate manually or use advanced algorithms that adjust it automatically.\n",
    "\n",
    "Now you understand how a model learns and adapts by updating its weights through gradient descent. Keep in mind that the gradient needs to be recalculated and reset at the start of each training epoch to continue refining the model effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
