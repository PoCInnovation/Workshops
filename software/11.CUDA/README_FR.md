# **Workshop 10 - CUDA et acc√©l√©ration mat√©rielle**

Durant ce workshop vous apprendrez √† utiliser CUDA, l'API d√©velopp√© par Nvidia permettant de r√©aliser des programmes qui utilisent les c≈ìurs des cartes graphiques.

Dans un premier temps, vous ferez en sorte d'ex√©cuter un Hello World sur plusieurs c≈ìurs d'un GPU en parall√®le.

Puis, vous d√©velopperez une application capable d'acc√©l√©rer l'addition de 2 listes de tr√®s grandes tailles.

Enfin, vous vous pencherez sur un probl√®me plus cons√©quent : transformer une image en couleur en une image en noir et blanc.

## **Step 0 - üöÄ Initialization**

Un repository a √©t√© cr√©√© pour vous gr√¢ce au lien d'invitation qui vous a √©t√© communiqu√© par mail. Lorsque vous *pushez* sur celui-ci des tests seront lanc√©s et vous permettrons de valider les prochaines √©tapes.

>üèÅ  **Avant de d√©buter, il est n√©cessaire que vous compreniez quelques termes.**

‚úîÔ∏è Comprendre l'execution sur CUDA

‚úîÔ∏è Comprendre la r√©partition de la m√©moire entre Host et Device

‚úîÔ∏è Comprendre la nomenclature sur CUDA

La syntaxe de CUDA est tr√®s similaire √† celle du C / C++.

> ‚ö†Ô∏è **Vous trouverez une explication d√©taill√©e [ici](https://dev.to/zenulabidin/an-overview-of-cuda-part-2-host-and-device-code-69d) des deux premiers points**.

1. Dans les ressources et durant vos recherches, le terme utilis√© pour designer le processeur (CPU) ainsi que la m√©moire classique (RAM) est **Host**.

2. Lorsqu'il est question de la carte graphique (GPU) ainsi que de la m√©moire de celle-ci (VRAM), le terme utilis√© est **Device**.

> ‚ö†Ô∏è **Vous trouverez une explication d√©taill√©e [ici](https://en.wikipedia.org/wiki/Thread_block_(CUDA_programming)) des deux derniers points**.

3. Afin de mieux organiser l'ex√©cution en parallele des c≈ìurs du GPU, appel√© un thread, ils sont repr√©sent√©s sur un rep√®re √† 3 dimensions.

4. Les threads sont regroup√©s en blocs, eux aussi repr√©sent√©s sur un rep√®re √† 3 dimensions : la Grille.

Voici un sch√©ma descriptif, car *une image vaut mieux que mille mots*, de l'organisation des threads en blocs dans une grille en 2 dimensions.

<div align="center">
    <img src="../../.github/assets/CUDAthreads.png" width=50%"/>
</div>

## **Step 1 - üëã Hello CUDA World**

> üö© **Premi√®re t√¢che : classique, mais efficace. Hello CUDA World.**

‚úîÔ∏è Ex√©cuter du code avec CUDA

‚úîÔ∏è Indexer des threads

‚úîÔ∏è Comprendre l'execution d'un [kernel](https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/)

Pour vous familiariser avec CUDA, vous allez commencer par lancer une fonction sur 2 threads.

Celle-ci devra afficher la cha√Æne de caract√®res `"Hello CUDA World {idx}"`, ou idx correspond √† l'index du thread.

> ‚ö†Ô∏è ***Voici les ressources dont vous aurez besoin*** :

- [L'indexation sur CUDA](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#built-in-variables).

- [Port√©e des fonctions ex√©cut√©s sur CUDA](https://stackoverflow.com/questions/12373940/difference-between-global-and-device-functions).

- [Lancement d'un kernel sur x blocs contenants y threads](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#kernels).

## **Step 2 - üîÄ Summing up two arrays**

> üö© **Seconde t√¢che : additionner 2 listes qui contiennent chacune 1 048 576 d'√©l√©ments.
> Le r√©sultat de l'addition doit √™tre contenu dans la seconde liste.**

‚úîÔ∏è Allouer dynamiquement de la m√©moire sur un GPU

‚úîÔ∏è Utiliser l'indexation des threads dans la logique d'un programme

‚úîÔ∏è Copier des donn√©es depuis le Host vers le Device

‚úîÔ∏è Copier des donn√©es depuis le Device vers le Host

‚úîÔ∏è Synchroniser l'ex√©cution des threads

*Votre CPU va vite, tr√®s vite, mais son nombre de c≈ìurs est limit√© (pas plus de 128 pour les meilleurs).
Vous allez donc utiliser le tr√®s grand nombre de c≈ìurs pr√©sents sur un GPU pour acc√©l√©rer l'ex√©cution d'un programme.*

> **Toutes les √©tapes sont d√©crites dans le code issu des [ressources](https://github.com/PoCInnovation/Workshops/blob/master/software/11.CUDA/CUDA_steps.zip) sous la forme de TODOs.**

> ‚ö†Ô∏è ***Voici les ressources dont vous aurez besoin*** :

- [Les fonctions permettant de g√©rer la m√©moire (CRTL + F -> memc.. ou mall.. üòâ)](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html).

- [Synchronisation des threads](https://www.google.com/).

## **Step 3 - üñºÔ∏è Image filter**

Bravo, vous √™tes d√©sormais √† l'aise avec CUDA ! Maintenant, voyons un exemple plus... compliqu√© üòµ.

> üö© **Troisi√®me t√¢che : parcourir une image en couleur et d√©terminer la valeur de gris de chaque pixel pour transformer l'image en noir et blanc.**

Pour cette derni√®re √©tape de ce workshop, une grande partie du code vous est volontairement donn√©.
De plus, vous n'aurez pas √† vous soucier de la m√©moire !

Vous devrez √©crire le corps de la fonction **`deviceKernel`** pr√©sente dans le fichier **`Image.cu`**.

Cette fonction est responsable de la transformation d'un pixel en couleur en un pixel en noir et blanc. Elle prend pour cela trois param√®tres :

- **`m_devicePixels`**, la liste de pixels.
- **`w`**, la taille en ordonn√©e de l'image.
- **`h`**, la taille en abscisse de l'image.

Vous devez :

- D√©terminer le nombre de threads lanc√©s.
- D√©terminer l'[ID](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#built-in-variables) du thread actuel.
- D√©terminer le nombre de pixels dans l'image.
- Pour un certain nombre de pixels, g√©n√©rer des valeurs al√©atoires pour les champs r, g et b contenues entre 0 et 255. Petite note, la fonction rand n'existe pas sur CUDA.
- Calculer la valeur de gris du pixel.

> ‚ö†Ô∏è ***Voici les ressources dont vous aurez besoin*** :

- [Vous avez dit al√©atoire ?](https://docs.nvidia.com/cuda/curand/index.html)

## Authors

[Luca Georges Francois](https://github.com/0xpanoramix)
